{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86193394-efc7-479f-b2a5-4a8423f97995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de células: 11534\n",
      "Total POSITIVAS: 4755\n",
      "Total NEGATIVAS: 6779\n",
      "\n",
      "[Resumo Inicial]\n",
      "Treino POSITIVO: 3328\n",
      "Treino NEGATIVO: 4745\n",
      "Validação: 1729  (Pos: 713, Neg: 1016)\n",
      "Teste: 1732  (Pos: 714, Neg: 1018)\n",
      "\n",
      "Total de imagens descartadas por estarem fora dos limites: 425\n",
      "\n",
      "[Resumo Final - Células Consideradas Após Descarte]\n",
      "Treino POSITIVO: 3300\n",
      "Treino NEGATIVO: 4505\n",
      "Validação POSITIVO: 709\n",
      "Validação NEGATIVO: 942\n",
      "Teste POSITIVO: 704\n",
      "Teste NEGATIVO: 949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 287\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Validação\u001b[39;00m\n\u001b[0;32m    286\u001b[0m gerar_csv_de_atributos(val_pos_dir, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_pos.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 287\u001b[0m gerar_csv_de_atributos(val_neg_dir, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_neg.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    288\u001b[0m juntar_csvs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_pos.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_neg.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Teste\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 251\u001b[0m, in \u001b[0;36mgerar_csv_de_atributos\u001b[1;34m(diretorio_imagens, label_binaria, output_csv)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nome_arquivo\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    250\u001b[0m     caminho_img \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(diretorio_imagens, nome_arquivo)\n\u001b[1;32m--> 251\u001b[0m     atributos \u001b[38;5;241m=\u001b[39m extrair_atributos(caminho_img)\n\u001b[0;32m    253\u001b[0m     nome_base \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(nome_arquivo)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    254\u001b[0m     partes \u001b[38;5;241m=\u001b[39m nome_base\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_celula_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 176\u001b[0m, in \u001b[0;36mextrair_atributos\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextrair_atributos\u001b[39m(image_path):\n\u001b[1;32m--> 176\u001b[0m     img \u001b[38;5;241m=\u001b[39m imread(image_path, as_gray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m     img_uint8 \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# --- Segmentação para atributos morfológicos ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\_shared\\utils.py:328\u001b[0m, in \u001b[0;36mdeprecate_parameter.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;66;03m# Assign old value to new one\u001b[39;00m\n\u001b[0;32m    326\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_name] \u001b[38;5;241m=\u001b[39m deprecated_value\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\io\\_io.py:82\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     79\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname, _hide_plugin_deprecation_warnings():\n\u001b[1;32m---> 82\u001b[0m     img \u001b[38;5;241m=\u001b[39m call_plugin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimread\u001b[39m\u001b[38;5;124m'\u001b[39m, fname, plugin\u001b[38;5;241m=\u001b[39mplugin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_args)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\_shared\\utils.py:538\u001b[0m, in \u001b[0;36mdeprecate_func.__call__.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m stacklevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stack_length(func) \u001b[38;5;241m-\u001b[39m stack_rank\n\u001b[0;32m    537\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(message, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[1;32m--> 538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\io\\manage_plugins.py:254\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(imageio_imread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRITEABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     13\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\v3.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_kwargs) \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(img_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\core\\imopen.py:196\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     plugin_instance \u001b[38;5;241m=\u001b[39m candidate_plugin(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InitializationError:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# file extension doesn't match file type\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\plugins\\pillow.py:104\u001b[0m, in \u001b[0;36mPillowPlugin.__init__\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mio_mode \u001b[38;5;241m==\u001b[39m IOMode\u001b[38;5;241m.\u001b[39mread:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(request\u001b[38;5;241m.\u001b[39mget_file()):\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;66;03m# Check if it is generally possible to read the image.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;66;03m# This will not read any data and merely try to find a\u001b[39;00m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;66;03m# compatible pillow plugin (ref: the pillow docs).\u001b[39;00m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnidentifiedImageError:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\core\\request.py:492\u001b[0m, in \u001b[0;36mRequest.get_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 492\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri_type \u001b[38;5;241m==\u001b[39m URI_ZIPPED:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# Get the correct filename\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     filename, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Caminhos\n",
    "base_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications_2classes.json\")\n",
    "output_dir_treino = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino\"\n",
    "output_dir_val = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao\"\n",
    "output_dir_teste = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/testeboundingbox/teste\"\n",
    "\n",
    "# Diretórios de saída\n",
    "train_neg_dir = os.path.join(output_dir_treino, \"treino-dir-negativo\")\n",
    "train_pos_dir = os.path.join(output_dir_treino, \"treino-dir-positivo\")\n",
    "val_pos_dir = os.path.join(output_dir_val, \"validacao-dir-positivo\")\n",
    "val_neg_dir = os.path.join(output_dir_val, \"validacao-dir-negativo\")\n",
    "test_pos_dir = os.path.join(output_dir_teste, \"teste-dir-positivo\")\n",
    "test_neg_dir = os.path.join(output_dir_teste, \"teste-dir-negativo\")\n",
    "\n",
    "os.makedirs(train_neg_dir, exist_ok=True)\n",
    "os.makedirs(train_pos_dir, exist_ok=True)\n",
    "os.makedirs(val_pos_dir, exist_ok=True)\n",
    "os.makedirs(val_neg_dir, exist_ok=True)\n",
    "os.makedirs(test_pos_dir, exist_ok=True)\n",
    "os.makedirs(test_neg_dir, exist_ok=True)\n",
    "\n",
    "# Carregar JSON\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extrair todas as células\n",
    "all_cells = []\n",
    "\n",
    "for img_data in data:\n",
    "    image_name = img_data[\"image_name\"]\n",
    "    for cell in img_data[\"classifications\"]:\n",
    "        all_cells.append({\n",
    "            \"image_name\": image_name,\n",
    "            \"cell_id\": cell[\"cell_id\"],\n",
    "            \"x\": cell[\"nucleus_x\"],\n",
    "            \"y\": cell[\"nucleus_y\"],\n",
    "            \"label\": cell[\"bethesda_system\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total de células: {len(all_cells)}\")\n",
    "\n",
    "# Separar por classe\n",
    "positive_cells = [c for c in all_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "negative_cells = [c for c in all_cells if c[\"label\"] == \"NEGATIVE\"]\n",
    "\n",
    "print(f\"Total POSITIVAS: {len(positive_cells)}\")\n",
    "print(f\"Total NEGATIVAS: {len(negative_cells)}\")\n",
    "\n",
    "# Embaralhar\n",
    "random.seed(42)\n",
    "random.shuffle(positive_cells)\n",
    "random.shuffle(negative_cells)\n",
    "\n",
    "# Divisão 70/15/15\n",
    "def split_data(cells):\n",
    "    total = len(cells)\n",
    "    n_train = int(0.7 * total)\n",
    "    n_val = int(0.15 * total)\n",
    "    train = cells[:n_train]\n",
    "    val = cells[n_train:n_train + n_val]\n",
    "    test = cells[n_train + n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "train_pos, val_pos, test_pos = split_data(positive_cells)\n",
    "train_neg, val_neg, test_neg = split_data(negative_cells)\n",
    "\n",
    "# Conjuntos finais\n",
    "val_cells = val_pos + val_neg\n",
    "test_cells = test_pos + test_neg\n",
    "\n",
    "# Embaralhar\n",
    "random.shuffle(val_cells)\n",
    "random.shuffle(test_cells)\n",
    "\n",
    "# Resumo\n",
    "print(f\"\\n[Resumo Inicial]\")\n",
    "print(f\"Treino POSITIVO: {len(train_pos)}\")\n",
    "print(f\"Treino NEGATIVO: {len(train_neg)}\")\n",
    "print(f\"Validação: {len(val_cells)}  (Pos: {len(val_pos)}, Neg: {len(val_neg)})\")\n",
    "print(f\"Teste: {len(test_cells)}  (Pos: {len(test_pos)}, Neg: {len(test_neg)})\")\n",
    "\n",
    "# Contadores\n",
    "descartadas = 0\n",
    "usadas = {\n",
    "    \"train_pos\": 0,\n",
    "    \"train_neg\": 0,\n",
    "    \"val_pos\": 0,\n",
    "    \"val_neg\": 0,\n",
    "    \"test_pos\": 0,\n",
    "    \"test_neg\": 0\n",
    "}\n",
    "\n",
    "def save_cropped(cell, image_dir, dest_dir, key_contador):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Imagem não encontrada: {cell['image_name']}\")\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao abrir {cell['image_name']}: {e}\")\n",
    "        descartadas += 1\n",
    "        return\n",
    "\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    crop_size = 70\n",
    "    half_crop = crop_size // 2\n",
    "\n",
    "    x1 = x - half_crop\n",
    "    y1 = y - half_crop\n",
    "    x2 = x + half_crop\n",
    "    y2 = y + half_crop\n",
    "\n",
    "    if x1 < 0 or y1 < 0 or x2 > image.width or y2 > image.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "\n",
    "    cropped = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "    name_prefix = os.path.splitext(cell[\"image_name\"])[0]\n",
    "    out_name = f\"{name_prefix}_celula_{cell['cell_id']}.png\"\n",
    "    save_path = os.path.join(dest_dir, out_name)\n",
    "    cropped.save(save_path)\n",
    "\n",
    "    usadas[key_contador] += 1\n",
    "\n",
    "# ---------- Loops para salvar imagens -------------\n",
    "for cell in train_pos:\n",
    "    save_cropped(cell, base_dir, train_pos_dir, \"train_pos\")\n",
    "for cell in train_neg:\n",
    "    save_cropped(cell, base_dir, train_neg_dir, \"train_neg\")\n",
    "for cell in val_pos:\n",
    "    save_cropped(cell, base_dir, val_pos_dir, \"val_pos\")\n",
    "for cell in val_neg:\n",
    "    save_cropped(cell, base_dir, val_neg_dir, \"val_neg\")\n",
    "for cell in test_pos:\n",
    "    save_cropped(cell, base_dir, test_pos_dir, \"test_pos\")\n",
    "for cell in test_neg:\n",
    "    save_cropped(cell, base_dir, test_neg_dir, \"test_neg\")\n",
    "\n",
    "# Imprimir número total de descartadas\n",
    "print(f\"\\nTotal de imagens descartadas por estarem fora dos limites: {descartadas}\")\n",
    "\n",
    "# Resumo final após descartes\n",
    "print(\"\\n[Resumo Final - Células Consideradas Após Descarte]\")\n",
    "print(f\"Treino POSITIVO: {usadas['train_pos']}\")\n",
    "print(f\"Treino NEGATIVO: {usadas['train_neg']}\")\n",
    "print(f\"Validação POSITIVO: {usadas['val_pos']}\")\n",
    "print(f\"Validação NEGATIVO: {usadas['val_neg']}\")\n",
    "print(f\"Teste POSITIVO: {usadas['test_pos']}\")\n",
    "print(f\"Teste NEGATIVO: {usadas['test_neg']}\")\n",
    "\n",
    "\n",
    "# ---------------------- ATRIBUTOS + CSV ----------------------\n",
    "def extrair_atributos(image_path):\n",
    "    img = imread(image_path, as_gray=True)\n",
    "    img_uint8 = (img * 255).astype(np.uint8)\n",
    "\n",
    "    # --- Segmentação para atributos morfológicos ---\n",
    "    try:\n",
    "        thresh = threshold_otsu(img)\n",
    "        binary = img > thresh\n",
    "        binary = morphology.remove_small_objects(binary, 30)\n",
    "        labeled = label(binary)\n",
    "        props = regionprops(labeled)\n",
    "\n",
    "        area = perimeter = eccentricity = circularity = elipticidade = 0\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area = p.area\n",
    "            perimeter = p.perimeter\n",
    "            eccentricity = p.eccentricity\n",
    "            circularity = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
    "            elipticidade = (p.major_axis_length / p.minor_axis_length) if p.minor_axis_length > 0 else 0\n",
    "    except Exception:\n",
    "        area = perimeter = eccentricity = circularity = elipticidade = 0\n",
    "\n",
    "    # --- Intensidade ---\n",
    "    mean_intensity = img.mean()\n",
    "    std_intensity = img.std()\n",
    "    skewness = skew(img.ravel())\n",
    "    kurt = kurtosis(img.ravel())\n",
    "    entropy_val = -np.sum(img * np.log2(img + 1e-10))\n",
    "\n",
    "    # --- GLCM ---\n",
    "    glcm = graycomatrix(img_uint8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "\n",
    "    # --- LBP ---\n",
    "    lbp = local_binary_pattern(img, P=8, R=1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "\n",
    "    # --- Haralick ---\n",
    "    haralick = mahotas.features.haralick(img_uint8, ignore_zeros=False)\n",
    "    haralick_mean = haralick.mean(axis=0)\n",
    "\n",
    "    # --- TAS (Threshold Adjacency Statistics) ---\n",
    "    try:\n",
    "        tas = mahotas.features.tas(img_uint8)\n",
    "    except Exception:\n",
    "        tas = [0] * 6\n",
    "        \n",
    "    # --- Momentos de Zernike ---\n",
    "    radius = min(img.shape) // 2\n",
    "    try:\n",
    "        zernike_moments = mahotas.features.zernike_moments(img_uint8, radius=radius, degree=8)\n",
    "    except Exception:\n",
    "        zernike_moments = [0] * 25  # degree 8 ≈ 25 momentos\n",
    "\n",
    "\n",
    "    return np.hstack([\n",
    "        area, perimeter, eccentricity, circularity, elipticidade,\n",
    "        mean_intensity, std_intensity, skewness, kurt, entropy_val,\n",
    "        contrast, correlation, energy, homogeneity,\n",
    "        lbp_hist,\n",
    "        haralick_mean,\n",
    "        tas,\n",
    "        zernike_moments\n",
    "    ])\n",
    "\n",
    "def gerar_csv_de_atributos(diretorio_imagens, label_binaria, output_csv):\n",
    "    linhas = []\n",
    "\n",
    "\n",
    "    for nome_arquivo in os.listdir(diretorio_imagens):\n",
    "        if nome_arquivo.endswith(\".png\"):\n",
    "            caminho_img = os.path.join(diretorio_imagens, nome_arquivo)\n",
    "            atributos = extrair_atributos(caminho_img)\n",
    "\n",
    "            nome_base = os.path.splitext(nome_arquivo)[0]\n",
    "            partes = nome_base.split(\"_celula_\")\n",
    "            image_name = partes[0]\n",
    "            cell_id = partes[1] if len(partes) > 1 else \"NA\"\n",
    "\n",
    "            linha = [image_name, cell_id] + list(atributos) + [label_binaria]\n",
    "            linhas.append(linha)\n",
    "\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f'feat_{i}' for i in range(len(linhas[0])-3)] + ['label']\n",
    "\n",
    "\n",
    "    # Normalização Min-Max apenas nas colunas de atributos (ignorando nome e label)\n",
    "    col_atributos = df.columns[2:-1]\n",
    "    scaler = MinMaxScaler()\n",
    "    df[col_atributos] = scaler.fit_transform(df[col_atributos])\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "def juntar_csvs(csv1, csv2, output_final):\n",
    "    df1 = pd.read_csv(csv1)\n",
    "    df2 = pd.read_csv(csv2)\n",
    "    df_final = pd.concat([df1, df2], ignore_index=True)\n",
    "    df_final = df_final.sample(frac=1, random_state=42)\n",
    "    df_final.to_csv(output_final, index=False)\n",
    "\n",
    "# Gerar atributos para treino\n",
    "gerar_csv_de_atributos(train_pos_dir, 1, \"train_pos.csv\")\n",
    "gerar_csv_de_atributos(train_neg_dir, 0, \"train_neg.csv\")\n",
    "juntar_csvs(\"train_pos.csv\", \"train_neg.csv\", \"train.csv\")\n",
    "\n",
    "# Validação\n",
    "gerar_csv_de_atributos(val_pos_dir, 1, \"val_pos.csv\")\n",
    "gerar_csv_de_atributos(val_neg_dir, 0, \"val_neg.csv\")\n",
    "juntar_csvs(\"val_pos.csv\", \"val_neg.csv\", \"val.csv\")\n",
    "\n",
    "# Teste\n",
    "gerar_csv_de_atributos(test_pos_dir, 1, \"test_pos.csv\")\n",
    "gerar_csv_de_atributos(test_neg_dir, 0, \"test_neg.csv\")\n",
    "juntar_csvs(\"test_pos.csv\", \"test_neg.csv\", \"test.csv\")\n",
    "\n",
    "print(\"\\nProcessamento completo: imagens salvas e atributos exportados.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc7dd5f-2c1c-4990-bc8b-7be928e1b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de células: 11534\n",
      "Intersecção treino/validação: set()\n",
      "Intersecção treino/teste: set()\n",
      "Intersecção validação/teste: set()\n",
      "\n",
      "Processamento completo: imagens salvas, aumentadas e atributos exportados.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Caminhos\n",
    "base_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications.json\")\n",
    "output_dir_treino = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino\"\n",
    "output_dir_val = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao\"\n",
    "output_dir_teste = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/testeboundingbox/teste\"\n",
    "\n",
    "# Diretórios de saída\n",
    "train_neg_dir = os.path.join(output_dir_treino, \"treino-dir-negativo\")\n",
    "train_pos_dir = os.path.join(output_dir_treino, \"treino-dir-positivo\")\n",
    "val_pos_dir = os.path.join(output_dir_val, \"validacao-dir-positivo\")\n",
    "val_neg_dir = os.path.join(output_dir_val, \"validacao-dir-negativo\")\n",
    "test_pos_dir = os.path.join(output_dir_teste, \"teste-dir-positivo\")\n",
    "test_neg_dir = os.path.join(output_dir_teste, \"teste-dir-negativo\")\n",
    "\n",
    "os.makedirs(train_neg_dir, exist_ok=True)\n",
    "os.makedirs(train_pos_dir, exist_ok=True)\n",
    "os.makedirs(val_pos_dir, exist_ok=True)\n",
    "os.makedirs(val_neg_dir, exist_ok=True)\n",
    "os.makedirs(test_pos_dir, exist_ok=True)\n",
    "os.makedirs(test_neg_dir, exist_ok=True)\n",
    "\n",
    "# Carregar JSON\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extrair todas as células\n",
    "all_cells = []\n",
    "for img_data in data:\n",
    "    image_name = img_data[\"image_name\"]\n",
    "    for cell in img_data[\"classifications\"]:\n",
    "        all_cells.append({\n",
    "            \"image_name\": image_name,\n",
    "            \"cell_id\": cell[\"cell_id\"],\n",
    "            \"x\": cell[\"nucleus_x\"],\n",
    "            \"y\": cell[\"nucleus_y\"],\n",
    "            \"label\": cell[\"bethesda_system\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total de células: {len(all_cells)}\")\n",
    "\n",
    "# Separar por classe\n",
    "positive_cells = [c for c in all_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "negative_cells = [c for c in all_cells if c[\"label\"] != \"POSITIVE\"]\n",
    "\n",
    "# Embaralhar imagens de forma global\n",
    "random.seed(42)\n",
    "\n",
    "# Agrupar todas as células por imagem\n",
    "imagem_to_celulas = {}\n",
    "for cell in all_cells:\n",
    "    imagem_to_celulas.setdefault(cell[\"image_name\"], []).append(cell)\n",
    "\n",
    "# Lista de imagens únicas\n",
    "imagens_unicas = list(imagem_to_celulas.keys())\n",
    "random.shuffle(imagens_unicas)\n",
    "\n",
    "# Divisão global por imagem\n",
    "n_total = len(imagens_unicas)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_imgs = imagens_unicas[:n_train]\n",
    "val_imgs = imagens_unicas[n_train:n_train + n_val]\n",
    "test_imgs = imagens_unicas[n_train + n_val:]\n",
    "\n",
    "# Agora alocamos células conforme a imagem associada\n",
    "train_cells = sum([imagem_to_celulas[img] for img in train_imgs], [])\n",
    "val_cells = sum([imagem_to_celulas[img] for img in val_imgs], [])\n",
    "test_cells = sum([imagem_to_celulas[img] for img in test_imgs], [])\n",
    "\n",
    "# Agora dividimos por classe\n",
    "train_pos = [c for c in train_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "train_neg = [c for c in train_cells if c[\"label\"] != \"POSITIVE\"]\n",
    "val_pos = [c for c in val_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "val_neg = [c for c in val_cells if c[\"label\"] != \"POSITIVE\"]\n",
    "test_pos = [c for c in test_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "test_neg = [c for c in test_cells if c[\"label\"] != \"POSITIVE\"]\n",
    "\n",
    "# Salvar imagens\n",
    "usadas = {k: 0 for k in [\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"]}\n",
    "descartadas = 0\n",
    "\n",
    "def save_cropped(cell, image_dir, dest_dir, key):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "    except:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    half_crop = 35\n",
    "    if x - half_crop < 0 or y - half_crop < 0 or x + half_crop > img.width or y + half_crop > img.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    crop = img.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "    name = f\"{os.path.splitext(cell['image_name'])[0]}_celula_{cell['cell_id']}.png\"\n",
    "    crop.save(os.path.join(dest_dir, name))\n",
    "    usadas[key] += 1\n",
    "\n",
    "for c in train_pos: save_cropped(c, base_dir, train_pos_dir, \"train_pos\")\n",
    "for c in train_neg: save_cropped(c, base_dir, train_neg_dir, \"train_neg\")\n",
    "for c in val_pos: save_cropped(c, base_dir, val_pos_dir, \"val_pos\")\n",
    "for c in val_neg: save_cropped(c, base_dir, val_neg_dir, \"val_neg\")\n",
    "for c in test_pos: save_cropped(c, base_dir, test_pos_dir, \"test_pos\")\n",
    "for c in test_neg: save_cropped(c, base_dir, test_neg_dir, \"test_neg\")\n",
    "\n",
    "# Aumento de dados - apenas treino POSITIVO\n",
    "def augment_and_save(image_path, name_base, dest_dir):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "        variants = [\n",
    "            img.rotate(15), img.rotate(-15),\n",
    "            img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "            img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "            ImageEnhance.Contrast(img).enhance(1.5)\n",
    "        ]\n",
    "        for i, aug in enumerate(variants):\n",
    "            output_path = os.path.normpath(os.path.join(dest_dir, f\"{name_base}_aug{i+1}.png\"))\n",
    "            aug.save(output_path)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for f in os.listdir(train_pos_dir):\n",
    "    if f.endswith(\".png\"):\n",
    "        augment_and_save(os.path.join(train_pos_dir, f), os.path.splitext(f)[0], train_pos_dir)\n",
    "\n",
    "# Extrair atributos\n",
    "def extrair_atributos(p):\n",
    "    img = imread(p, as_gray=True)\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "    try:\n",
    "        bin = morphology.remove_small_objects(img > threshold_otsu(img), 30)\n",
    "        props = regionprops(label(bin))\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area, perim = p.area, p.perimeter\n",
    "            ecc = p.eccentricity\n",
    "            circ = 4*np.pi*area/(perim**2) if perim > 0 else 0\n",
    "            elip = p.major_axis_length/p.minor_axis_length if p.minor_axis_length > 0 else 0\n",
    "        else:\n",
    "            area = perim = ecc = circ = elip = 0\n",
    "    except:\n",
    "        area = perim = ecc = circ = elip = 0\n",
    "    mean, std, skw, krt = img.mean(), img.std(), skew(img.ravel()), kurtosis(img.ravel())\n",
    "    ent = -np.sum(img * np.log2(img + 1e-10))\n",
    "    glcm = graycomatrix(img_u8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0,0]\n",
    "    corr = graycoprops(glcm, 'correlation')[0,0]\n",
    "    energy = graycoprops(glcm, 'energy')[0,0]\n",
    "    homog = graycoprops(glcm, 'homogeneity')[0,0]\n",
    "    lbp = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0,11), density=True)\n",
    "    hrlk = mahotas.features.haralick(img_u8).mean(axis=0)\n",
    "    try: tas = mahotas.features.tas(img_u8)\n",
    "    except: tas = [0]*6\n",
    "    try: zern = mahotas.features.zernike_moments(img_u8, radius=min(img.shape)//2, degree=8)\n",
    "    except: zern = [0]*25\n",
    "    return np.hstack([area, perim, ecc, circ, elip, mean, std, skw, krt, ent,\n",
    "                      contrast, corr, energy, homog, lbp_hist, hrlk, tas, zern])\n",
    "    \n",
    "def gerar_df_features(diretorio, label):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base) > 1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0]) - 3)] + [\"label\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def gerar_csv(diretorio, label, output):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base)>1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0])-3)] + [\"label\"]\n",
    "    col_attr = df.columns[2:-1]\n",
    "    df[col_attr] = MinMaxScaler().fit_transform(df[col_attr])\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "def juntar_csvs(csv1, csv2, out):\n",
    "    df = pd.concat([pd.read_csv(csv1), pd.read_csv(csv2)], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "# Gera arquivos finais\n",
    "gerar_csv(train_pos_dir, 1, \"train_pos.csv\")\n",
    "gerar_csv(train_neg_dir, 0, \"train_neg.csv\")\n",
    "juntar_csvs(\"train_pos.csv\", \"train_neg.csv\", \"train.csv\")\n",
    "\n",
    "gerar_csv(val_pos_dir, 1, \"val_pos.csv\")\n",
    "gerar_csv(val_neg_dir, 0, \"val_neg.csv\")\n",
    "juntar_csvs(\"val_pos.csv\", \"val_neg.csv\", \"val.csv\")\n",
    "\n",
    "gerar_csv(test_pos_dir, 1, \"test_pos.csv\")\n",
    "gerar_csv(test_neg_dir, 0, \"test_neg.csv\")\n",
    "juntar_csvs(\"test_pos.csv\", \"test_neg.csv\", \"test.csv\")\n",
    "\n",
    "# Garante que não há imagens iguais entre os conjuntos\n",
    "train_imgs = set([c['image_name'] for c in train_pos + train_neg])\n",
    "val_imgs = set([c['image_name'] for c in val_pos + val_neg])\n",
    "test_imgs = set([c['image_name'] for c in test_pos + test_neg])\n",
    "\n",
    "print(\"Intersecção treino/validação:\", train_imgs & val_imgs)\n",
    "print(\"Intersecção treino/teste:\", train_imgs & test_imgs)\n",
    "print(\"Intersecção validação/teste:\", val_imgs & test_imgs)\n",
    "\n",
    "\n",
    "print(\"\\nProcessamento completo: imagens salvas, aumentadas e atributos exportados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08798594-58d4-4af4-9837-b185a143ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem inicial - Descartadas: 0\n",
      "Contagem após salvar imagens - Descartadas: 398\n",
      "Usadas por conjunto: {'train_pos': 3329, 'train_neg': 4447, 'val_pos': 545, 'val_neg': 512, 'test_pos': 839, 'test_neg': 805}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 244\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessamento finalizado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[12], line 226\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    223\u001b[0m     augmentar_imagens_treino_positivo()\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Gerar CSVs\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m gerar_csv(train_pos_dir, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pos.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m gerar_csv(train_neg_dir, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_neg.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m gerar_csv(val_pos_dir, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_pos.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 171\u001b[0m, in \u001b[0;36mgerar_csv\u001b[1;34m(diretorio, label, output)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arq\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    170\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(diretorio, arq))\n\u001b[1;32m--> 171\u001b[0m     feat \u001b[38;5;241m=\u001b[39m extrair_atributos(path)\n\u001b[0;32m    172\u001b[0m     base \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(arq)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_celula_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m     linhas\u001b[38;5;241m.\u001b[39mappend([base[\u001b[38;5;241m0\u001b[39m], base[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(base)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(feat) \u001b[38;5;241m+\u001b[39m [label])\n",
      "Cell \u001b[1;32mIn[12], line 134\u001b[0m, in \u001b[0;36mextrair_atributos\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextrair_atributos\u001b[39m(p):\n\u001b[1;32m--> 134\u001b[0m     img \u001b[38;5;241m=\u001b[39m imread(p, as_gray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m     img_u8 \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\_shared\\utils.py:328\u001b[0m, in \u001b[0;36mdeprecate_parameter.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;66;03m# Assign old value to new one\u001b[39;00m\n\u001b[0;32m    326\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_name] \u001b[38;5;241m=\u001b[39m deprecated_value\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\io\\_io.py:82\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     79\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname, _hide_plugin_deprecation_warnings():\n\u001b[1;32m---> 82\u001b[0m     img \u001b[38;5;241m=\u001b[39m call_plugin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimread\u001b[39m\u001b[38;5;124m'\u001b[39m, fname, plugin\u001b[38;5;241m=\u001b[39mplugin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_args)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\_shared\\utils.py:538\u001b[0m, in \u001b[0;36mdeprecate_func.__call__.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m stacklevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stack_length(func) \u001b[38;5;241m-\u001b[39m stack_rank\n\u001b[0;32m    537\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(message, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[1;32m--> 538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\io\\manage_plugins.py:254\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(imageio_imread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRITEABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     13\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\v3.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_kwargs) \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(img_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\core\\imopen.py:196\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     plugin_instance \u001b[38;5;241m=\u001b[39m candidate_plugin(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InitializationError:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# file extension doesn't match file type\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\plugins\\pillow.py:104\u001b[0m, in \u001b[0;36mPillowPlugin.__init__\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mio_mode \u001b[38;5;241m==\u001b[39m IOMode\u001b[38;5;241m.\u001b[39mread:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(request\u001b[38;5;241m.\u001b[39mget_file()):\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;66;03m# Check if it is generally possible to read the image.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;66;03m# This will not read any data and merely try to find a\u001b[39;00m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;66;03m# compatible pillow plugin (ref: the pillow docs).\u001b[39;00m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnidentifiedImageError:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imageio\\core\\request.py:492\u001b[0m, in \u001b[0;36mRequest.get_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 492\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri_type \u001b[38;5;241m==\u001b[39m URI_ZIPPED:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# Get the correct filename\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     filename, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Configurações de diretórios ---\n",
    "base_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications.json\")\n",
    "output_dir_treino = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino\"\n",
    "output_dir_val = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao\"\n",
    "output_dir_teste = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/testeboundingbox/teste\"\n",
    "\n",
    "train_neg_dir = os.path.join(output_dir_treino, \"treino-dir-negativo\")\n",
    "train_pos_dir = os.path.join(output_dir_treino, \"treino-dir-positivo\")\n",
    "val_pos_dir = os.path.join(output_dir_val, \"validacao-dir-positivo\")\n",
    "val_neg_dir = os.path.join(output_dir_val, \"validacao-dir-negativo\")\n",
    "test_pos_dir = os.path.join(output_dir_teste, \"teste-dir-positivo\")\n",
    "test_neg_dir = os.path.join(output_dir_teste, \"teste-dir-negativo\")\n",
    "\n",
    "# --- Funções ---\n",
    "\n",
    "def criar_diretorios():\n",
    "    for d in [train_neg_dir, train_pos_dir, val_pos_dir, val_neg_dir, test_pos_dir, test_neg_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def carregar_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extrair_celulas(data):\n",
    "    all_cells = []\n",
    "    for img_data in data:\n",
    "        image_name = img_data[\"image_name\"]\n",
    "        for cell in img_data[\"classifications\"]:\n",
    "            all_cells.append({\n",
    "                \"image_name\": image_name,\n",
    "                \"cell_id\": cell[\"cell_id\"],\n",
    "                \"x\": cell[\"nucleus_x\"],\n",
    "                \"y\": cell[\"nucleus_y\"],\n",
    "                \"label\": cell[\"bethesda_system\"]\n",
    "            })\n",
    "    return all_cells\n",
    "\n",
    "def dividir_dados_por_imagem(all_cells, seed=42):\n",
    "    random.seed(seed)\n",
    "    imagem_to_celulas = {}\n",
    "    for cell in all_cells:\n",
    "        imagem_to_celulas.setdefault(cell[\"image_name\"], []).append(cell)\n",
    "    imagens_unicas = list(imagem_to_celulas.keys())\n",
    "    random.shuffle(imagens_unicas)\n",
    "    n_total = len(imagens_unicas)\n",
    "    n_train = round(0.7 * n_total)\n",
    "    n_val = round(0.15 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    train_imgs = imagens_unicas[:n_train]\n",
    "    val_imgs = imagens_unicas[n_train:n_train + n_val]\n",
    "    test_imgs = imagens_unicas[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "    train_cells = sum([imagem_to_celulas[img] for img in train_imgs], [])\n",
    "    val_cells = sum([imagem_to_celulas[img] for img in val_imgs], [])\n",
    "    test_cells = sum([imagem_to_celulas[img] for img in test_imgs], [])\n",
    "    return train_cells, val_cells, test_cells\n",
    "\n",
    "\n",
    "def dividir_por_classe(cells):\n",
    "    pos = [c for c in cells if c[\"label\"] == \"POSITIVE\"]\n",
    "    neg = [c for c in cells if c[\"label\"] != \"POSITIVE\"]\n",
    "    return pos, neg\n",
    "\n",
    "descartadas = 0\n",
    "usadas = {k: 0 for k in [\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"]}\n",
    "\n",
    "def save_cropped(cell, image_dir, dest_dir, key):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "    except:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    half_crop = 35\n",
    "    if x - half_crop < 0 or y - half_crop < 0 or x + half_crop > img.width or y + half_crop > img.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    crop = img.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "    name = f\"{os.path.splitext(cell['image_name'])[0]}_celula_{cell['cell_id']}.png\"\n",
    "    crop.save(os.path.join(dest_dir, name))\n",
    "    usadas[key] += 1\n",
    "\n",
    "def salvar_imagens_por_conjunto(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg):\n",
    "    for c in train_pos: save_cropped(c, base_dir, train_pos_dir, \"train_pos\")\n",
    "    for c in train_neg: save_cropped(c, base_dir, train_neg_dir, \"train_neg\")\n",
    "    for c in val_pos: save_cropped(c, base_dir, val_pos_dir, \"val_pos\")\n",
    "    for c in val_neg: save_cropped(c, base_dir, val_neg_dir, \"val_neg\")\n",
    "    for c in test_pos: save_cropped(c, base_dir, test_pos_dir, \"test_pos\")\n",
    "    for c in test_neg: save_cropped(c, base_dir, test_neg_dir, \"test_neg\")\n",
    "\n",
    "def augment_and_save(image_path, name_base, dest_dir):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "        variants = [\n",
    "            img.rotate(15), img.rotate(-15),\n",
    "            img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "            img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "            ImageEnhance.Contrast(img).enhance(1.5)\n",
    "        ]\n",
    "        for i, aug in enumerate(variants):\n",
    "            output_path = os.path.normpath(os.path.join(dest_dir, f\"{name_base}_aug{i+1}.png\"))\n",
    "            aug.save(output_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def augmentar_imagens_treino_positivo():\n",
    "    for f in os.listdir(train_pos_dir):\n",
    "        if f.endswith(\".png\"):\n",
    "            augment_and_save(os.path.join(train_pos_dir, f), os.path.splitext(f)[0], train_pos_dir)\n",
    "\n",
    "def extrair_atributos(p):\n",
    "    img = imread(p, as_gray=True)\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "    try:\n",
    "        bin = morphology.remove_small_objects(img > threshold_otsu(img), 30)\n",
    "        props = regionprops(label(bin))\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area, perim = p.area, p.perimeter\n",
    "            ecc = p.eccentricity\n",
    "            circ = 4*np.pi*area/(perim**2) if perim > 0 else 0\n",
    "            elip = p.major_axis_length/p.minor_axis_length if p.minor_axis_length > 0 else 0\n",
    "        else:\n",
    "            area = perim = ecc = circ = elip = 0\n",
    "    except:\n",
    "        area = perim = ecc = circ = elip = 0\n",
    "    mean, std, skw, krt = img.mean(), img.std(), skew(img.ravel()), kurtosis(img.ravel())\n",
    "    ent = -np.sum(img * np.log2(img + 1e-10))\n",
    "    glcm = graycomatrix(img_u8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0,0]\n",
    "    corr = graycoprops(glcm, 'correlation')[0,0]\n",
    "    energy = graycoprops(glcm, 'energy')[0,0]\n",
    "    homog = graycoprops(glcm, 'homogeneity')[0,0]\n",
    "    lbp = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0,11), density=True)\n",
    "    hrlk = mahotas.features.haralick(img_u8).mean(axis=0)\n",
    "    try: tas = mahotas.features.tas(img_u8)\n",
    "    except: tas = [0]*6\n",
    "    try: zern = mahotas.features.zernike_moments(img_u8, radius=min(img.shape)//2, degree=8)\n",
    "    except: zern = [0]*25\n",
    "    return np.hstack([area, perim, ecc, circ, elip, mean, std, skw, krt, ent,\n",
    "                      contrast, corr, energy, homog, lbp_hist, hrlk, tas, zern])\n",
    "\n",
    "def gerar_csv(diretorio, label, output):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base)>1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0])-3)] + [\"label\"]\n",
    "    col_attr = df.columns[2:-1]\n",
    "    df[col_attr] = MinMaxScaler().fit_transform(df[col_attr])\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "def juntar_csvs(csv1, csv2, out):\n",
    "    df = pd.concat([pd.read_csv(csv1), pd.read_csv(csv2)], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def verificar_interseccao_conjuntos(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg):\n",
    "    train_imgs = set([c['image_name'] for c in train_pos + train_neg])\n",
    "    val_imgs = set([c['image_name'] for c in val_pos + val_neg])\n",
    "    test_imgs = set([c['image_name'] for c in test_pos + test_neg])\n",
    "    print(\"Intersecção treino/validação:\", train_imgs & val_imgs)\n",
    "    print(\"Intersecção treino/teste:\", train_imgs & test_imgs)\n",
    "    print(\"Intersecção validação/teste:\", val_imgs & test_imgs)\n",
    "\n",
    "def main():\n",
    "    criar_diretorios()\n",
    "    data = carregar_json(json_path)\n",
    "    all_cells = extrair_celulas(data)\n",
    "    train_cells, val_cells, test_cells = dividir_dados_por_imagem(all_cells)\n",
    "    train_pos, train_neg = dividir_por_classe(train_cells)\n",
    "    val_pos, val_neg = dividir_por_classe(val_cells)\n",
    "    test_pos, test_neg = dividir_por_classe(test_cells)\n",
    "\n",
    "    # Balancear validação\n",
    "    n_val = min(len(val_pos), len(val_neg))\n",
    "    val_pos = random.sample(val_pos, n_val)\n",
    "    val_neg = random.sample(val_neg, n_val)\n",
    "    \n",
    "    # Balancear teste\n",
    "    n_test = min(len(test_pos), len(test_neg))\n",
    "    test_pos = random.sample(test_pos, n_test)\n",
    "    test_neg = random.sample(test_neg, n_test)\n",
    "    \n",
    "    \n",
    "    print(f\"Contagem inicial - Descartadas: {descartadas}\")\n",
    "    \n",
    "    salvar_imagens_por_conjunto(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg)\n",
    "    \n",
    "    print(f\"Contagem após salvar imagens - Descartadas: {descartadas}\")\n",
    "    print(\"Usadas por conjunto:\", usadas)\n",
    "    \n",
    "    # Data augmentation somente para treino positivo\n",
    "    aumentar = True\n",
    "    if aumentar:\n",
    "        augmentar_imagens_treino_positivo()\n",
    "    \n",
    "    # Gerar CSVs\n",
    "    gerar_csv(train_pos_dir, 1, \"train_pos.csv\")\n",
    "    gerar_csv(train_neg_dir, 0, \"train_neg.csv\")\n",
    "    gerar_csv(val_pos_dir, 1, \"val_pos.csv\")\n",
    "    gerar_csv(val_neg_dir, 0, \"val_neg.csv\")\n",
    "    gerar_csv(test_pos_dir, 1, \"test_pos.csv\")\n",
    "    gerar_csv(test_neg_dir, 0, \"test_neg.csv\")\n",
    "    \n",
    "    # Juntar CSVs treino, validação e teste (positivo + negativo)\n",
    "    juntar_csvs(\"train_pos.csv\", \"train_neg.csv\", \"train.csv\")\n",
    "    juntar_csvs(\"val_pos.csv\", \"val_neg.csv\", \"val.csv\")\n",
    "    juntar_csvs(\"test_pos.csv\", \"test_neg.csv\", \"test.csv\")\n",
    "    \n",
    "    # Verificar interseções (não devem existir imagens em mais de um conjunto)\n",
    "    verificar_interseccao_conjuntos(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg)\n",
    "    \n",
    "    print(\"Processamento finalizado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d64912-00c7-4905-8b71-969e43bf142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Total de células descartadas: 388\n",
      "[INFO] train_pos: 3329\n",
      "[INFO] train_neg: 4447\n",
      "[INFO] val_pos: 545\n",
      "[INFO] val_neg: 519\n",
      "[INFO] test_pos: 839\n",
      "[INFO] test_neg: 808\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Configurações de diretórios ---\n",
    "base_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications.json\")\n",
    "output_dir_treino = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino\"\n",
    "output_dir_val = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao\"\n",
    "output_dir_teste = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/testeboundingbox/teste\"\n",
    "\n",
    "train_neg_dir = os.path.join(output_dir_treino, \"treino-dir-negativo\")\n",
    "train_pos_dir = os.path.join(output_dir_treino, \"treino-dir-positivo\")\n",
    "val_pos_dir = os.path.join(output_dir_val, \"validacao-dir-positivo\")\n",
    "val_neg_dir = os.path.join(output_dir_val, \"validacao-dir-negativo\")\n",
    "test_pos_dir = os.path.join(output_dir_teste, \"teste-dir-positivo\")\n",
    "test_neg_dir = os.path.join(output_dir_teste, \"teste-dir-negativo\")\n",
    "\n",
    "# --- Funções ---\n",
    "\n",
    "def criar_diretorios():\n",
    "    for d in [train_neg_dir, train_pos_dir, val_pos_dir, val_neg_dir, test_pos_dir, test_neg_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def carregar_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extrair_celulas(data):\n",
    "    all_cells = []\n",
    "    for img_data in data:\n",
    "        image_name = img_data[\"image_name\"]\n",
    "        for cell in img_data[\"classifications\"]:\n",
    "            all_cells.append({\n",
    "                \"image_name\": image_name,\n",
    "                \"cell_id\": cell[\"cell_id\"],\n",
    "                \"x\": cell[\"nucleus_x\"],\n",
    "                \"y\": cell[\"nucleus_y\"],\n",
    "                \"label\": cell[\"bethesda_system\"]\n",
    "            })\n",
    "    return all_cells\n",
    "\n",
    "def dividir_dados_por_imagem(all_cells, seed=42):\n",
    "    random.seed(seed)\n",
    "    imagem_to_celulas = {}\n",
    "    for cell in all_cells:\n",
    "        imagem_to_celulas.setdefault(cell[\"image_name\"], []).append(cell)\n",
    "    imagens_unicas = list(imagem_to_celulas.keys())\n",
    "    random.shuffle(imagens_unicas)\n",
    "    n_total = len(imagens_unicas)\n",
    "    n_train = round(0.7 * n_total)\n",
    "    n_val = round(0.15 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    train_imgs = imagens_unicas[:n_train]\n",
    "    val_imgs = imagens_unicas[n_train:n_train + n_val]\n",
    "    test_imgs = imagens_unicas[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "    train_cells = sum([imagem_to_celulas[img] for img in train_imgs], [])\n",
    "    val_cells = sum([imagem_to_celulas[img] for img in val_imgs], [])\n",
    "    test_cells = sum([imagem_to_celulas[img] for img in test_imgs], [])\n",
    "    return train_cells, val_cells, test_cells\n",
    "\n",
    "def dividir_por_classe(cells):\n",
    "    pos = [c for c in cells if c[\"label\"] == \"POSITIVE\"]\n",
    "    neg = [c for c in cells if c[\"label\"] != \"POSITIVE\"]\n",
    "    return pos, neg\n",
    "\n",
    "def balancear_positivo_negativo(pos_list, neg_list, seed=42):\n",
    "    random.seed(seed)\n",
    "    n = min(len(pos_list), len(neg_list))\n",
    "    return random.sample(pos_list, n), random.sample(neg_list, n)\n",
    "\n",
    "descartadas = 0\n",
    "usadas = {k: 0 for k in [\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"]}\n",
    "\n",
    "def save_cropped(cell, image_dir, dest_dir, key):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "    except:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    half_crop = 35\n",
    "    if x - half_crop < 0 or y - half_crop < 0 or x + half_crop > img.width or y + half_crop > img.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    crop = img.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "    name = f\"{os.path.splitext(cell['image_name'])[0]}_celula_{cell['cell_id']}.png\"\n",
    "    crop.save(os.path.join(dest_dir, name))\n",
    "    usadas[key] += 1\n",
    "\n",
    "def salvar_imagens_por_conjunto(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg):\n",
    "    for c in train_pos: save_cropped(c, base_dir, train_pos_dir, \"train_pos\")\n",
    "    for c in train_neg: save_cropped(c, base_dir, train_neg_dir, \"train_neg\")\n",
    "    for c in val_pos: save_cropped(c, base_dir, val_pos_dir, \"val_pos\")\n",
    "    for c in val_neg: save_cropped(c, base_dir, val_neg_dir, \"val_neg\")\n",
    "    for c in test_pos: save_cropped(c, base_dir, test_pos_dir, \"test_pos\")\n",
    "    for c in test_neg: save_cropped(c, base_dir, test_neg_dir, \"test_neg\")\n",
    "\n",
    "def augment_and_save(image_path, name_base, dest_dir):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "        variants = [\n",
    "            img.rotate(15), img.rotate(-15),\n",
    "            img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "            img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "            ImageEnhance.Contrast(img).enhance(1.5)\n",
    "        ]\n",
    "        for i, aug in enumerate(variants):\n",
    "            output_path = os.path.normpath(os.path.join(dest_dir, f\"{name_base}_aug{i+1}.png\"))\n",
    "            aug.save(output_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def augmentar_imagens_treino_positivo():\n",
    "    for f in os.listdir(train_pos_dir):\n",
    "        if f.endswith(\".png\"):\n",
    "            augment_and_save(os.path.join(train_pos_dir, f), os.path.splitext(f)[0], train_pos_dir)\n",
    "\n",
    "def extrair_atributos(p):\n",
    "    img = imread(p, as_gray=True)\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "    try:\n",
    "        bin = morphology.remove_small_objects(img > threshold_otsu(img), 30)\n",
    "        props = regionprops(label(bin))\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area, perim = p.area, p.perimeter\n",
    "            ecc = p.eccentricity\n",
    "            circ = 4*np.pi*area/(perim**2) if perim > 0 else 0\n",
    "            elip = p.major_axis_length/p.minor_axis_length if p.minor_axis_length > 0 else 0\n",
    "        else:\n",
    "            area = perim = ecc = circ = elip = 0\n",
    "    except:\n",
    "        area = perim = ecc = circ = elip = 0\n",
    "    mean, std, skw, krt = img.mean(), img.std(), skew(img.ravel()), kurtosis(img.ravel())\n",
    "    ent = -np.sum(img * np.log2(img + 1e-10))\n",
    "    glcm = graycomatrix(img_u8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0,0]\n",
    "    corr = graycoprops(glcm, 'correlation')[0,0]\n",
    "    energy = graycoprops(glcm, 'energy')[0,0]\n",
    "    homog = graycoprops(glcm, 'homogeneity')[0,0]\n",
    "    lbp = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0,11), density=True)\n",
    "    hrlk = mahotas.features.haralick(img_u8).mean(axis=0)\n",
    "    try: tas = mahotas.features.tas(img_u8)\n",
    "    except: tas = [0]*6\n",
    "    try: zern = mahotas.features.zernike_moments(img_u8, radius=min(img.shape)//2, degree=8)\n",
    "    except: zern = [0]*25\n",
    "    return np.hstack([area, perim, ecc, circ, elip, mean, std, skw, krt, ent,\n",
    "                      contrast, corr, energy, homog, lbp_hist, hrlk, tas, zern])\n",
    "\n",
    "def gerar_csv(diretorio, label, output):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base)>1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0])-3)] + [\"label\"]\n",
    "    col_attr = df.columns[2:-1]\n",
    "    df[col_attr] = MinMaxScaler().fit_transform(df[col_attr])\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "def juntar_csvs(csv1, csv2, out):\n",
    "    df = pd.concat([pd.read_csv(csv1), pd.read_csv(csv2)], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def verificar_interseccao_conjuntos(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg):\n",
    "    train_imgs = set([c['image_name'] for c in train_pos + train_neg])\n",
    "    val_imgs = set([c['image_name'] for c in val_pos + val_neg])\n",
    "    test_imgs = set([c['image_name'] for c in test_pos + test_neg])\n",
    "    print(\"Intersecção treino/validação:\", train_imgs & val_imgs)\n",
    "    print(\"Intersecção treino/teste:\", train_imgs & test_imgs)\n",
    "    print(\"Intersecção validação/teste:\", val_imgs & test_imgs)\n",
    "\n",
    "def main():\n",
    "    criar_diretorios()\n",
    "    data = carregar_json(json_path)\n",
    "    all_cells = extrair_celulas(data)\n",
    "    train_cells, val_cells, test_cells = dividir_dados_por_imagem(all_cells)\n",
    "\n",
    "    # Divide cada conjunto em positivo e negativo\n",
    "    train_pos, train_neg = dividir_por_classe(train_cells)\n",
    "    val_pos, val_neg = dividir_por_classe(val_cells)\n",
    "    test_pos, test_neg = dividir_por_classe(test_cells)\n",
    "\n",
    "    # Balanceia validação e teste\n",
    "    val_pos, val_neg = balancear_positivo_negativo(val_pos, val_neg)\n",
    "    test_pos, test_neg = balancear_positivo_negativo(test_pos, test_neg)\n",
    "\n",
    "    verificar_interseccao_conjuntos(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg)\n",
    "    salvar_imagens_por_conjunto(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg)\n",
    "    augmentar_imagens_treino_positivo()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Criar diretórios\n",
    "    criar_diretorios()\n",
    "\n",
    "    # 2. Carregar JSON com anotações\n",
    "    dados_json = carregar_json(json_path)\n",
    "\n",
    "    # 3. Extrair células com labels\n",
    "    todas_celulas = extrair_celulas(dados_json)\n",
    "\n",
    "    # 4. Dividir dados por imagem (para evitar vazamento entre treino/val/teste)\n",
    "    train_cells, val_cells, test_cells = dividir_dados_por_imagem(todas_celulas)\n",
    "\n",
    "    # 5. Separar cada conjunto por classe\n",
    "    train_pos, train_neg = dividir_por_classe(train_cells)\n",
    "    val_pos, val_neg = dividir_por_classe(val_cells)\n",
    "    test_pos, test_neg = dividir_por_classe(test_cells)\n",
    "\n",
    "    # 6. BALANCEAR VALIDAÇÃO E TESTE (positivo = negativo)\n",
    "    val_pos, val_neg = balancear_positivo_negativo(val_pos, val_neg)\n",
    "    test_pos, test_neg = balancear_positivo_negativo(test_pos, test_neg)\n",
    "\n",
    "    # 7. Salvar imagens recortadas\n",
    "    salvar_imagens_por_conjunto(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg)\n",
    "\n",
    "    # 8. Aumentar dados apenas do treino POSITIVO\n",
    "    augmentar_imagens_treino_positivo()\n",
    "\n",
    "    # 9. Gerar CSVs\n",
    "    gerar_csv(train_pos_dir, \"POSITIVE\", \"csv_treino_pos.csv\")\n",
    "    gerar_csv(train_neg_dir, \"NEGATIVE\", \"csv_treino_neg.csv\")\n",
    "    gerar_csv(val_pos_dir, \"POSITIVE\", \"csv_validacao_pos.csv\")\n",
    "    gerar_csv(val_neg_dir, \"NEGATIVE\", \"csv_validacao_neg.csv\")\n",
    "    gerar_csv(test_pos_dir, \"POSITIVE\", \"csv_teste_pos.csv\")\n",
    "    gerar_csv(test_neg_dir, \"NEGATIVE\", \"csv_teste_neg.csv\")\n",
    "\n",
    "    # 10. Juntar positivos e negativos em conjuntos completos\n",
    "    juntar_csvs(\"csv_treino_pos.csv\", \"csv_treino_neg.csv\", \"csv_treino_completo.csv\")\n",
    "    juntar_csvs(\"csv_validacao_pos.csv\", \"csv_validacao_neg.csv\", \"csv_validacao_completo.csv\")\n",
    "    juntar_csvs(\"csv_teste_pos.csv\", \"csv_teste_neg.csv\", \"csv_teste_completo.csv\")\n",
    "\n",
    "    # 11. Exibir contagens\n",
    "    print(f\"\\n[INFO] Total de células descartadas: {descartadas}\")\n",
    "    for k, v in usadas.items():\n",
    "        print(f\"[INFO] {k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d888dd3e-0d24-4c7f-a852-e6449a6181da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumentando classe POSITIVE com 1205 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                            | 121/3300 [00:01<00:32, 99.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído. Total gerado: 1205\n",
      " Processamento completo com balanceamento, augmentations e normalização segura.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ========== CAMINHOS ==========\n",
    "#base_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/base\"\n",
    "#json_path = os.path.join(base_dir, \"classifications_2classes.json\")\n",
    "#output_dir_treino = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino/2classes\"\n",
    "#output_dir_val = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao/2classes/\"\n",
    "#output_dir_teste = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/teste/teste/2classes/\"\n",
    "base_dir = \"E:/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications_2classes.json\")\n",
    "output_dir_treino = \"E:/datasets/imagens/treino/treino/2classes\"\n",
    "output_dir_val = \"E:/datasets/imagens/validacao/validacao/2classes/\"\n",
    "output_dir_teste = \"E:/datasets/imagens/teste/teste/2classes/\"\n",
    "\n",
    "# ========== CRIAR DIRETÓRIOS ==========\n",
    "train_neg_dir_rgb = os.path.join(output_dir_treino, \"treino-dir-negativo-rgb\")\n",
    "train_pos_dir_rgb = os.path.join(output_dir_treino, \"treino-dir-positivo-rgb\")\n",
    "val_pos_dir_rgb = os.path.join(output_dir_val, \"validacao-dir-positivo-rgb\")\n",
    "val_neg_dir_rgb = os.path.join(output_dir_val, \"validacao-dir-negativo-rgb\")\n",
    "test_pos_dir_rgb = os.path.join(output_dir_teste, \"teste-dir-positivo-rgb\")\n",
    "test_neg_dir_rgb = os.path.join(output_dir_teste, \"teste-dir-negativo-rgb\")\n",
    "train_neg_dir = os.path.join(output_dir_treino, \"treino-dir-negativo\")\n",
    "train_pos_dir = os.path.join(output_dir_treino, \"treino-dir-positivo\")\n",
    "val_pos_dir = os.path.join(output_dir_val, \"validacao-dir-positivo\")\n",
    "val_neg_dir = os.path.join(output_dir_val, \"validacao-dir-negativo\")\n",
    "test_pos_dir = os.path.join(output_dir_teste, \"teste-dir-positivo\")\n",
    "test_neg_dir = os.path.join(output_dir_teste, \"teste-dir-negativo\")\n",
    "\n",
    "for d in [train_neg_dir, train_pos_dir, val_pos_dir, val_neg_dir, test_pos_dir, test_neg_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "# Criar diretórios RGB\n",
    "for d in [train_neg_dir_rgb, train_pos_dir_rgb, val_pos_dir_rgb, val_neg_dir_rgb, test_pos_dir_rgb, test_neg_dir_rgb]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ========== CARREGAR JSON ==========\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ========== EXTRATO DE CÉLULAS ==========\n",
    "all_cells = []\n",
    "for img_data in data:\n",
    "    image_name = img_data[\"image_name\"]\n",
    "    for cell in img_data[\"classifications\"]:\n",
    "        all_cells.append({\n",
    "            \"image_name\": image_name,\n",
    "            \"cell_id\": cell[\"cell_id\"],\n",
    "            \"x\": cell[\"nucleus_x\"],\n",
    "            \"y\": cell[\"nucleus_y\"],\n",
    "            \"label\": cell[\"bethesda_system\"]\n",
    "        })\n",
    "\n",
    "# ========== DIVISÃO ENTRE POS/NEG ==========\n",
    "positive_cells = [c for c in all_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "negative_cells = [c for c in all_cells if c[\"label\"] != \"POSITIVE\"]\n",
    "\n",
    "# ========== DIVISÃO TREINO/VAL/TEST POR IMAGEM ==========\n",
    "random.seed(42)\n",
    "# Shuffle global\n",
    "random.seed(42)\n",
    "\n",
    "# Separar por classe\n",
    "positive_cells = [c for c in all_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "negative_cells = [c for c in all_cells if c[\"label\"] != \"POSITIVE\"]\n",
    "\n",
    "random.shuffle(positive_cells)\n",
    "random.shuffle(negative_cells)\n",
    "\n",
    "# Split por classe individualmente\n",
    "def split_data(cells):\n",
    "    total = len(cells)\n",
    "    n_train = int(0.7 * total)\n",
    "    n_val = int(0.15 * total)\n",
    "    train = cells[:n_train]\n",
    "    val = cells[n_train:n_train + n_val]\n",
    "    test = cells[n_train + n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "# Aplicar split\n",
    "train_pos, val_pos, test_pos = split_data(positive_cells)\n",
    "train_neg, val_neg, test_neg = split_data(negative_cells)\n",
    "\n",
    "\n",
    "# ========== SALVAR RECORTES ==========\n",
    "usadas = {k: 0 for k in [\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"]}\n",
    "descartadas = 0\n",
    "gray_dirs = {\n",
    "    \"train_pos\": train_pos_dir,\n",
    "    \"train_neg\": train_neg_dir,\n",
    "    \"val_pos\": val_pos_dir,\n",
    "    \"val_neg\": val_neg_dir,\n",
    "    \"test_pos\": test_pos_dir,\n",
    "    \"test_neg\": test_neg_dir\n",
    "}\n",
    "\n",
    "rgb_dirs = {\n",
    "    \"train_pos\": train_pos_dir_rgb,\n",
    "    \"train_neg\": train_neg_dir_rgb,\n",
    "    \"val_pos\": val_pos_dir_rgb,\n",
    "    \"val_neg\": val_neg_dir_rgb,\n",
    "    \"test_pos\": test_pos_dir_rgb,\n",
    "    \"test_neg\": test_neg_dir_rgb\n",
    "}\n",
    "\n",
    "def save_cropped_dual(cell, image_dir, key):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        # Abrir em RGB e em escala de cinza\n",
    "        img_rgb = Image.open(image_path).convert(\"RGB\")\n",
    "        img_gray = img_rgb.convert(\"L\")\n",
    "    except:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    half_crop = 35\n",
    "    if x - half_crop < 0 or y - half_crop < 0 or x + half_crop > img_rgb.width or y + half_crop > img_rgb.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "\n",
    "    # Recortar ambas as versões\n",
    "    crop_rgb = img_rgb.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "    crop_gray = img_gray.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "\n",
    "    name = f\"{os.path.splitext(cell['image_name'])[0]}_celula_{cell['cell_id']}.png\"\n",
    "\n",
    "    # Salvar\n",
    "    crop_rgb.save(os.path.join(rgb_dirs[key], name))\n",
    "    crop_gray.save(os.path.join(gray_dirs[key], name))\n",
    "    usadas[key] += 1\n",
    "\n",
    "for c in train_pos: save_cropped_dual(c, base_dir, \"train_pos\")\n",
    "for c in train_neg: save_cropped_dual(c, base_dir, \"train_neg\")\n",
    "for c in val_pos: save_cropped_dual(c, base_dir, \"val_pos\")\n",
    "for c in val_neg: save_cropped_dual(c, base_dir, \"val_neg\")\n",
    "for c in test_pos: save_cropped_dual(c, base_dir, \"test_pos\")\n",
    "for c in test_neg: save_cropped_dual(c, base_dir, \"test_neg\")\n",
    "\n",
    "\n",
    "# ========== TRANSFORMAÇÕES ==========\n",
    "def apply_augmentations(img):\n",
    "    return [\n",
    "        # Rotações em ângulos fixos (15, 90°, 180° e 270°)\n",
    "        img.rotate(15),\n",
    "        img.rotate(90),\n",
    "        img.rotate(180),\n",
    "        img.rotate(270),\n",
    "        # Espelhamento horizontal e vertifcal\n",
    "        img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "        img.transpose(Image.FLIP_TOP_BOTTOM),\n",
    "        # Ajustes de contraste e nitidez\n",
    "        ImageEnhance.Contrast(img).enhance(1.5),\n",
    "        ImageEnhance.Sharpness(img).enhance(2),\n",
    "        # Filtros de desfoque, como o desfoque Gaussiano e o filtro da mediana\n",
    "        img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "        img.filter(ImageFilter.MedianFilter(size=3)),\n",
    "    ]\n",
    "\n",
    "def balancear_treinamento_automaticamente(positivos_dir, negativos_dir):\n",
    "    pos_files = [f for f in os.listdir(positivos_dir) if f.endswith(\".png\")]\n",
    "    neg_files = [f for f in os.listdir(negativos_dir) if f.endswith(\".png\")]\n",
    "    qtd_pos, qtd_neg = len(pos_files), len(neg_files)\n",
    "\n",
    "    if qtd_pos < qtd_neg:\n",
    "        deficit = qtd_neg - qtd_pos\n",
    "        base_dir = positivos_dir\n",
    "        base_files = pos_files\n",
    "        classe = \"POSITIVE\"\n",
    "    elif qtd_neg < qtd_pos:\n",
    "        deficit = qtd_pos - qtd_neg\n",
    "        base_dir = negativos_dir\n",
    "        base_files = neg_files\n",
    "        classe = \"NEGATIVE\"\n",
    "    else:\n",
    "        print(\"Classes já estão balanceadas.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Aumentando classe {classe} com {deficit} imagens...\")\n",
    "\n",
    "    contador = 0\n",
    "    for f in tqdm(base_files):\n",
    "        if contador >= deficit:\n",
    "            break\n",
    "        path = os.path.join(base_dir, f)\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"L\")\n",
    "            for i, aug in enumerate(apply_augmentations(img)):\n",
    "                if contador >= deficit:\n",
    "                    break\n",
    "                out_name = f\"{os.path.splitext(f)[0]}_aug{i+1}.png\"\n",
    "                aug.save(os.path.join(base_dir, out_name))\n",
    "                contador += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"Aumento de dados concluído. Total gerado: {contador}\")\n",
    "\n",
    "balancear_treinamento_automaticamente(train_pos_dir, train_neg_dir)\n",
    "\n",
    "# ========== EXTRAÇÃO DE ATRIBUTOS ==========\n",
    "def extrair_atributos(p):\n",
    "    img = imread(p, as_gray=True)\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "\n",
    "    try:\n",
    "        bin = morphology.remove_small_objects(img > threshold_otsu(img), 30)\n",
    "        props = regionprops(label(bin))\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area, perim = p.area, p.perimeter\n",
    "            ecc = p.eccentricity\n",
    "            circ = 4*np.pi*area/(perim**2) if perim > 0 else 0\n",
    "            elip = p.major_axis_length/p.minor_axis_length if p.minor_axis_length > 0 else 0\n",
    "        else:\n",
    "            area = perim = ecc = circ = elip = 0\n",
    "    except:\n",
    "        area = perim = ecc = circ = elip = 0\n",
    "\n",
    "    mean, std, skw, krt = img.mean(), img.std(), skew(img.ravel()), kurtosis(img.ravel())\n",
    "    ent = -np.sum(img * np.log2(img + 1e-10))\n",
    "\n",
    "    glcm = graycomatrix(img_u8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    corr = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "    homog = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "\n",
    "    lbp = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "\n",
    "    hrlk = mahotas.features.haralick(img_u8).mean(axis=0)\n",
    "    tas = mahotas.features.tas(img_u8)\n",
    "    zern = mahotas.features.zernike_moments(img_u8, radius=min(img.shape)//2, degree=8)\n",
    "    \n",
    "    # ===== DESCRITORES DE FOURIER =====\n",
    "    fft = np.fft.fft2(img)\n",
    "    fft_shift = np.fft.fftshift(fft)\n",
    "    magnitude_spectrum = np.abs(fft_shift)\n",
    "\n",
    "    # Normalização para evitar overflow\n",
    "    magnitude_spectrum /= (magnitude_spectrum.max() + 1e-10)\n",
    "\n",
    "    # Estatísticas do espectro\n",
    "    fft_mean = magnitude_spectrum.mean()\n",
    "    fft_std = magnitude_spectrum.std()\n",
    "    fft_energy = np.sum(magnitude_spectrum**2)\n",
    "    fft_entropy = -np.sum(magnitude_spectrum * np.log2(magnitude_spectrum + 1e-10))\n",
    "\n",
    "    return np.hstack([\n",
    "        area, perim, ecc, circ, elip,\n",
    "        mean, std, skw, krt, ent,\n",
    "        contrast, corr, energy, homog,\n",
    "        lbp_hist, hrlk, tas, zern,  fft_mean, fft_std, fft_energy, fft_entropy\n",
    "    ])\n",
    "\n",
    "# ========== CSV COM NORMALIZAÇÃO BASEADA NO TREINO ==========\n",
    "def gerar_df_csv(diretorio, label):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base)>1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0])-3)] + [\"label\"]\n",
    "    return df\n",
    "\n",
    "def normalizar_e_salvar(df_train, df_val, df_test):\n",
    "    col_attr = df_train.columns[2:-1]\n",
    "    scaler = MinMaxScaler().fit(df_train[col_attr])\n",
    "    df_train[col_attr] = scaler.transform(df_train[col_attr])\n",
    "    df_val[col_attr] = scaler.transform(df_val[col_attr])\n",
    "    df_test[col_attr] = scaler.transform(df_test[col_attr])\n",
    "    df_train.to_csv(\"train_2classes.csv\", index=False)\n",
    "    df_val.to_csv(\"val_2classes.csv\", index=False)\n",
    "    df_test.to_csv(\"test_2classes.csv\", index=False)\n",
    "\n",
    "df_train = pd.concat([\n",
    "    gerar_df_csv(train_pos_dir, 1),\n",
    "    gerar_df_csv(train_neg_dir, 0)\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_val = pd.concat([\n",
    "    gerar_df_csv(val_pos_dir, 1),\n",
    "    gerar_df_csv(val_neg_dir, 0)\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_test = pd.concat([\n",
    "    gerar_df_csv(test_pos_dir, 1),\n",
    "    gerar_df_csv(test_neg_dir, 0)\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "normalizar_e_salvar(df_train, df_val, df_test)\n",
    "\n",
    "print(\" Processamento completo com balanceamento, augmentations e normalização segura.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6005b-1d4b-4f7f-baca-90bab09f1396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
