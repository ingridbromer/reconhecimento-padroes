{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68c1749-85bc-4383-9c7d-1ca090fa84b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumentando classe POSITIVE com 2238 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 224/2234 [00:02<00:21, 94.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe POSITIVE. Total gerado: 2238\n",
      "Classe NEGATIVE já está balanceada.\n",
      "Aumentando classe LIMITROFE com 3410 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▎                                                     | 341/1062 [00:03<00:07, 99.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe LIMITROFE. Total gerado: 3410\n",
      " Processamento completo com balanceamento, augmentations e normalização segura.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ========== CAMINHOS ==========\n",
    "#base_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/base\"\n",
    "#json_path = os.path.join(base_dir, \"classifications_3classes.json\")\n",
    "#output_dir_treino = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino/3classes/\"\n",
    "#output_dir_val = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao/3classes/\"\n",
    "#output_dir_teste = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/teste/teste/3classes/\"\n",
    "base_dir = \"E:/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications_3classes.json\")\n",
    "output_dir_treino = \"E:/datasets/imagens/treino/treino/3classes/\"\n",
    "output_dir_val = \"E:/datasets/imagens/validacao/validacao/3classes/\"\n",
    "output_dir_teste = \"E:/datasets/imagens/teste/teste/3classes/\"\n",
    "\n",
    "# ========== CRIAR DIRETÓRIOS ==========\n",
    "train_neg_dir_rgb = os.path.join(output_dir_treino, \"treino-dir-negativo-rgb\")\n",
    "train_pos_dir_rgb = os.path.join(output_dir_treino, \"treino-dir-positivo-rgb\")\n",
    "val_pos_dir_rgb = os.path.join(output_dir_val, \"validacao-dir-positivo-rgb\")\n",
    "val_neg_dir_rgb = os.path.join(output_dir_val, \"validacao-dir-negativo-rgb\")\n",
    "test_pos_dir_rgb = os.path.join(output_dir_teste, \"teste-dir-positivo-rgb\")\n",
    "test_neg_dir_rgb = os.path.join(output_dir_teste, \"teste-dir-negativo-rgb\")\n",
    "train_lim_dir_rgb = os.path.join(output_dir_treino, \"treino-dir-limitrofe-rgb\")\n",
    "val_lim_dir_rgb = os.path.join(output_dir_val, \"validacao-dir-limitrofe-rgb\")\n",
    "test_lim_dir_rgb = os.path.join(output_dir_teste, \"teste-dir-limitrofe-rgb\")\n",
    "train_neg_dir = os.path.join(output_dir_treino, \"treino-dir-negativo\")\n",
    "train_pos_dir = os.path.join(output_dir_treino, \"treino-dir-positivo\")\n",
    "val_pos_dir = os.path.join(output_dir_val, \"validacao-dir-positivo\")\n",
    "val_neg_dir = os.path.join(output_dir_val, \"validacao-dir-negativo\")\n",
    "test_pos_dir = os.path.join(output_dir_teste, \"teste-dir-positivo\")\n",
    "test_neg_dir = os.path.join(output_dir_teste, \"teste-dir-negativo\")\n",
    "train_lim_dir = os.path.join(output_dir_treino, \"treino-dir-limitrofe\")\n",
    "val_lim_dir = os.path.join(output_dir_val, \"validacao-dir-limitrofe\")\n",
    "test_lim_dir = os.path.join(output_dir_teste, \"teste-dir-limitrofe\")\n",
    "\n",
    "for d in [train_neg_dir, train_pos_dir, val_pos_dir, val_neg_dir, test_pos_dir, test_neg_dir, train_lim_dir, val_lim_dir, test_lim_dir ]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "# Criar diretórios RGB\n",
    "for d in [train_neg_dir_rgb, train_pos_dir_rgb, val_pos_dir_rgb, val_neg_dir_rgb, test_pos_dir_rgb, test_neg_dir_rgb, train_lim_dir_rgb, val_lim_dir_rgb, test_lim_dir_rgb ]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ========== CARREGAR JSON ==========\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ========== EXTRATO DE CÉLULAS ==========\n",
    "all_cells = []\n",
    "for img_data in data:\n",
    "    image_name = img_data[\"image_name\"]\n",
    "    for cell in img_data[\"classifications\"]:\n",
    "        all_cells.append({\n",
    "            \"image_name\": image_name,\n",
    "            \"cell_id\": cell[\"cell_id\"],\n",
    "            \"x\": cell[\"nucleus_x\"],\n",
    "            \"y\": cell[\"nucleus_y\"],\n",
    "            \"label\": cell[\"bethesda_system\"]\n",
    "        })\n",
    "\n",
    "# ========== DIVISÃO ENTRE POS/NEG ==========\n",
    "positive_cells = [c for c in all_cells if c[\"label\"] == \"POSITIVE\"]\n",
    "negative_cells = [c for c in all_cells if c[\"label\"] == \"NEGATIVE\"]\n",
    "limitrofe_cells = [c for c in all_cells if c[\"label\"] == \"LIMITROFE\"]\n",
    "\n",
    "# ========== DIVISÃO TREINO/VAL/TEST POR IMAGEM ==========\n",
    "random.seed(42)\n",
    "\n",
    "random.shuffle(positive_cells)\n",
    "random.shuffle(negative_cells)\n",
    "random.shuffle(limitrofe_cells)\n",
    "\n",
    "# Split por classe individualmente\n",
    "def split_data(cells):\n",
    "    total = len(cells)\n",
    "    n_train = int(0.7 * total)\n",
    "    n_val = int(0.15 * total)\n",
    "    train = cells[:n_train]\n",
    "    val = cells[n_train:n_train + n_val]\n",
    "    test = cells[n_train + n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "train_pos, val_pos, test_pos = split_data(positive_cells)\n",
    "train_neg, val_neg, test_neg = split_data(negative_cells)\n",
    "train_lim, val_lim, test_lim = split_data(limitrofe_cells)\n",
    "\n",
    "\n",
    "# ========== SALVAR RECORTES ==========\n",
    "usadas = {k: 0 for k in [\n",
    "    \"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\",\n",
    "    \"train_lim\", \"val_lim\", \"test_lim\"\n",
    "]}\n",
    "\n",
    "descartadas = 0\n",
    "gray_dirs = {\n",
    "    \"train_pos\": train_pos_dir,\n",
    "    \"train_neg\": train_neg_dir,\n",
    "    \"val_pos\": val_pos_dir,\n",
    "    \"val_neg\": val_neg_dir,\n",
    "    \"test_pos\": test_pos_dir,\n",
    "    \"test_neg\": test_neg_dir,\n",
    "    \"train_lim\": train_lim_dir,\n",
    "    \"val_lim\": val_lim_dir,\n",
    "    \"test_lim\": test_lim_dir\n",
    "}\n",
    "\n",
    "rgb_dirs = {\n",
    "    \"train_pos\": train_pos_dir_rgb,\n",
    "    \"train_neg\": train_neg_dir_rgb,\n",
    "    \"val_pos\": val_pos_dir_rgb,\n",
    "    \"val_neg\": val_neg_dir_rgb,\n",
    "    \"test_pos\": test_pos_dir_rgb,\n",
    "    \"test_neg\": test_neg_dir_rgb,\n",
    "    \"train_lim\": train_lim_dir_rgb,\n",
    "    \"val_lim\": val_lim_dir_rgb,\n",
    "    \"test_lim\": test_lim_dir_rgb\n",
    "    \n",
    "}\n",
    "\n",
    "def save_cropped_dual(cell, image_dir, key):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        # Abrir em RGB e em escala de cinza\n",
    "        img_rgb = Image.open(image_path).convert(\"RGB\")\n",
    "        img_gray = img_rgb.convert(\"L\")\n",
    "    except:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    half_crop = 35\n",
    "    if x - half_crop < 0 or y - half_crop < 0 or x + half_crop > img_rgb.width or y + half_crop > img_rgb.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "\n",
    "    # Recortar ambas as versões\n",
    "    crop_rgb = img_rgb.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "    crop_gray = img_gray.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "\n",
    "    name = f\"{os.path.splitext(cell['image_name'])[0]}_celula_{cell['cell_id']}.png\"\n",
    "\n",
    "    # Salvar\n",
    "    crop_rgb.save(os.path.join(rgb_dirs[key], name))\n",
    "    crop_gray.save(os.path.join(gray_dirs[key], name))\n",
    "    usadas[key] += 1\n",
    "\n",
    "for c in train_pos: save_cropped_dual(c, base_dir, \"train_pos\")\n",
    "for c in train_neg: save_cropped_dual(c, base_dir, \"train_neg\")\n",
    "for c in val_pos: save_cropped_dual(c, base_dir, \"val_pos\")\n",
    "for c in val_neg: save_cropped_dual(c, base_dir, \"val_neg\")\n",
    "for c in test_pos: save_cropped_dual(c, base_dir, \"test_pos\")\n",
    "for c in test_neg: save_cropped_dual(c, base_dir, \"test_neg\")\n",
    "for c in train_lim: save_cropped_dual(c, base_dir, \"train_lim\")\n",
    "for c in val_lim: save_cropped_dual(c, base_dir, \"val_lim\")\n",
    "for c in test_lim: save_cropped_dual(c, base_dir, \"test_lim\")\n",
    "\n",
    "\n",
    "\n",
    "# ========== TRANSFORMAÇÕES ==========\n",
    "def apply_augmentations(img):\n",
    "    return [\n",
    "        img.rotate(90),\n",
    "        img.rotate(180),\n",
    "        img.rotate(270),\n",
    "        img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "        img.transpose(Image.FLIP_TOP_BOTTOM),\n",
    "        ImageEnhance.Contrast(img).enhance(1.5),\n",
    "        ImageEnhance.Sharpness(img).enhance(2),\n",
    "        img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "        img.filter(ImageFilter.MedianFilter(size=3)),\n",
    "        img.rotate(15)\n",
    "    ]\n",
    "def balancear_treinamento_automaticamente(positivos_dir, negativos_dir, limitrofes_dir):\n",
    "    pos_files = [f for f in os.listdir(positivos_dir) if f.endswith(\".png\")]\n",
    "    neg_files = [f for f in os.listdir(negativos_dir) if f.endswith(\".png\")]\n",
    "    lim_files = [f for f in os.listdir(limitrofes_dir) if f.endswith(\".png\")]\n",
    "\n",
    "    qtd_pos, qtd_neg, qtd_lim = len(pos_files), len(neg_files), len(lim_files)\n",
    "\n",
    "    # Identificar a quantidade máxima entre as classes\n",
    "    max_qtd = max(qtd_pos, qtd_neg, qtd_lim)\n",
    "\n",
    "    # Lista de classes e seus dados\n",
    "    classes = [\n",
    "        (\"POSITIVE\", qtd_pos, positivos_dir, pos_files),\n",
    "        (\"NEGATIVE\", qtd_neg, negativos_dir, neg_files),\n",
    "        (\"LIMITROFE\", qtd_lim, limitrofes_dir, lim_files),\n",
    "    ]\n",
    "\n",
    "    # Para cada classe com quantidade menor que o máximo, aplicar aumento\n",
    "    for classe, qtd, base_dir, base_files in classes:\n",
    "        if qtd < max_qtd:\n",
    "            deficit = max_qtd - qtd\n",
    "            print(f\"Aumentando classe {classe} com {deficit} imagens...\")\n",
    "\n",
    "            contador = 0\n",
    "            for f in tqdm(base_files):\n",
    "                if contador >= deficit:\n",
    "                    break\n",
    "                path = os.path.join(base_dir, f)\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"L\")\n",
    "                    for i, aug in enumerate(apply_augmentations(img)):\n",
    "                        if contador >= deficit:\n",
    "                            break\n",
    "                        out_name = f\"{os.path.splitext(f)[0]}_aug{i+1}.png\"\n",
    "                        aug.save(os.path.join(base_dir, out_name))\n",
    "                        contador += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar {f}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"Aumento de dados concluído para classe {classe}. Total gerado: {contador}\")\n",
    "        else:\n",
    "            print(f\"Classe {classe} já está balanceada.\")\n",
    "            \n",
    "balancear_treinamento_automaticamente(train_pos_dir, train_neg_dir, train_lim_dir)\n",
    "# ========== EXTRAÇÃO DE ATRIBUTOS ==========\n",
    "def extrair_atributos(p):\n",
    "    img = imread(p, as_gray=True)\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "\n",
    "    try:\n",
    "        bin = morphology.remove_small_objects(img > threshold_otsu(img), 30)\n",
    "        props = regionprops(label(bin))\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area, perim = p.area, p.perimeter\n",
    "            ecc = p.eccentricity\n",
    "            circ = 4*np.pi*area/(perim**2) if perim > 0 else 0\n",
    "            elip = p.major_axis_length/p.minor_axis_length if p.minor_axis_length > 0 else 0\n",
    "        else:\n",
    "            area = perim = ecc = circ = elip = 0\n",
    "    except:\n",
    "        area = perim = ecc = circ = elip = 0\n",
    "\n",
    "    mean, std, skw, krt = img.mean(), img.std(), skew(img.ravel()), kurtosis(img.ravel())\n",
    "    ent = -np.sum(img * np.log2(img + 1e-10))\n",
    "\n",
    "    glcm = graycomatrix(img_u8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    corr = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "    homog = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "\n",
    "    lbp = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "\n",
    "    hrlk = mahotas.features.haralick(img_u8).mean(axis=0)\n",
    "    tas = mahotas.features.tas(img_u8)\n",
    "    zern = mahotas.features.zernike_moments(img_u8, radius=min(img.shape)//2, degree=8)\n",
    "    \n",
    "    # ===== DESCRITORES DE FOURIER =====\n",
    "    fft = np.fft.fft2(img)\n",
    "    fft_shift = np.fft.fftshift(fft)\n",
    "    magnitude_spectrum = np.abs(fft_shift)\n",
    "\n",
    "    # Normalização para evitar overflow\n",
    "    magnitude_spectrum /= (magnitude_spectrum.max() + 1e-10)\n",
    "\n",
    "    # Estatísticas do espectro\n",
    "    fft_mean = magnitude_spectrum.mean()\n",
    "    fft_std = magnitude_spectrum.std()\n",
    "    fft_energy = np.sum(magnitude_spectrum**2)\n",
    "    fft_entropy = -np.sum(magnitude_spectrum * np.log2(magnitude_spectrum + 1e-10))\n",
    "\n",
    "    return np.hstack([\n",
    "        area, perim, ecc, circ, elip,\n",
    "        mean, std, skw, krt, ent,\n",
    "        contrast, corr, energy, homog,\n",
    "        lbp_hist, hrlk, tas, zern,  fft_mean, fft_std, fft_energy, fft_entropy\n",
    "    ])\n",
    "\n",
    "# ========== CSV COM NORMALIZAÇÃO BASEADA NO TREINO ==========\n",
    "def gerar_df_csv(diretorio, label):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base)>1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0])-3)] + [\"label\"]\n",
    "    return df\n",
    "\n",
    "def normalizar_e_salvar(df_train, df_val, df_test):\n",
    "    col_attr = df_train.columns[2:-1]\n",
    "    scaler = MinMaxScaler().fit(df_train[col_attr])\n",
    "    df_train[col_attr] = scaler.transform(df_train[col_attr])\n",
    "    df_val[col_attr] = scaler.transform(df_val[col_attr])\n",
    "    df_test[col_attr] = scaler.transform(df_test[col_attr])\n",
    "    df_train.to_csv(\"train_3classes.csv\", index=False)\n",
    "    df_val.to_csv(\"val_3classes.csv\", index=False)\n",
    "    df_test.to_csv(\"test_3classes.csv\", index=False)\n",
    "\n",
    "df_train = pd.concat([\n",
    "    gerar_df_csv(train_lim_dir, 2),\n",
    "    gerar_df_csv(train_pos_dir, 1),\n",
    "    gerar_df_csv(train_neg_dir, 0)\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_val = pd.concat([\n",
    "    gerar_df_csv(val_lim_dir, 2),\n",
    "    gerar_df_csv(val_pos_dir, 1),\n",
    "    gerar_df_csv(val_neg_dir, 0)\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_test = pd.concat([\n",
    "    gerar_df_csv(test_lim_dir, 2),\n",
    "    gerar_df_csv(test_pos_dir, 1),\n",
    "    gerar_df_csv(test_neg_dir, 0)\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "normalizar_e_salvar(df_train, df_val, df_test)\n",
    "\n",
    "print(\" Processamento completo com balanceamento, augmentations e normalização segura.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e231e-f2c7-41d1-afba-a19302fdc55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
