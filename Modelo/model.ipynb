{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91da82c3-6b6f-4593-832b-77d420999b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from torch import einsum\n",
    "\n",
    "# --- Transformer blocks (iguais)\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=6, dim_head=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), qkv)\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "# --- EfficientNetB0 \n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.features = model.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.out_channels = 1280\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# --- ConTrans final com EfficientNetB0 + ViT + atributos\n",
    "class ConTrans(nn.Module):\n",
    "    def __init__(self, attr_dim=120, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hiperparâmetros\n",
    "        n_feats = 64\n",
    "        n_heads = 4\n",
    "        n_layers = 4\n",
    "        dim_head = n_feats // n_heads\n",
    "        expansion_ratio = 4\n",
    "        dropout_rate = 0.1\n",
    "        image_size = 70\n",
    "        patch_size = 5\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "        nn.Conv2d(3, n_feats, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(n_feats),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(0.1)\n",
    "        )\n",
    "\n",
    "        # Embedding de patches\n",
    "        self.patch_embedding = nn.Conv2d(n_feats, n_feats, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, n_feats))\n",
    "        self.embedding_dim = n_feats\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                PreNorm(n_feats, SelfAttention(n_feats, n_heads, dim_head, dropout_rate)),\n",
    "                PreNorm(n_feats, FeedForward(n_feats, n_feats * expansion_ratio, dropout_rate))\n",
    "            ]) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # EfficientNetB0 adaptada\n",
    "        self.effnet_branch = EfficientNetB0()\n",
    "        self.effnet_out_channels = self.effnet_branch.out_channels\n",
    "\n",
    "        # Classificador final\n",
    "        fusion_dim = self.embedding_dim + self.effnet_out_channels\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fusion_dim + attr_dim, 256),  # entrada = concatenação de todos os vetores e \n",
    "                                                    # redução da dimensionalidade para um vetor menor\n",
    "            nn.ReLU(),                              # ativação não-linear\n",
    "            nn.Dropout(0.5),                        # regularização\n",
    "            nn.Linear(256, num_classes)             # saída final com número de classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attr=None):\n",
    "        # Stem\n",
    "        x_stem = self.stem(x)  # [B, 64, 70, 70]\n",
    "\n",
    "        # ViT branch\n",
    "        patches = self.patch_embedding(x_stem)  # [B, C, H', W']\n",
    "        b, c, h, w = patches.shape\n",
    "        patches = patches.flatten(2).transpose(1, 2)  # [B, N, C]\n",
    "        patches += self.pos_embedding[:, :patches.size(1)]\n",
    "\n",
    "        for attn, ff in self.transformer:\n",
    "            patches = attn(patches) + patches\n",
    "            patches = ff(patches) + patches\n",
    "\n",
    "        vit_out = patches.mean(dim=1)  # [B, 64]\n",
    "\n",
    "        # EfficientNet branch\n",
    "        effnet_out = self.effnet_branch(x)  # [B, 1280]\n",
    "\n",
    "        # Fusão\n",
    "        fusion = torch.cat([vit_out, effnet_out], dim=1)  # [B, 1344]\n",
    "        if attr is not None:\n",
    "            fusion = torch.cat([fusion, attr], dim=1)      # [B, 1344 +119]\n",
    "\n",
    "        return self.fc(fusion)  # [B, num_classes]\n",
    "\n",
    "def get_ConTrans_func_by_name(name):\n",
    "    if name == \"ConTrans\":\n",
    "        return lambda deploy=False: ConTrans()\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo '{name}' não reconhecido\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5966ae-7d96-4f80-acf5-69c388c66739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
