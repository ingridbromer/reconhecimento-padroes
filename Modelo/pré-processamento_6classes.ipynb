{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d38cc7-1241-4d1f-9c41-f64f1990e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Células descartadas: 425\n",
      "Classe NEGATIVE já está balanceada.\n",
      "Aumentando classe ASC-H com 3838 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████▋                                | 384/644 [00:04<00:02, 86.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe ASC-H. Total gerado: 3838\n",
      "Aumentando classe ASC-US com 4065 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████  | 407/417 [00:05<00:00, 78.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe ASC-US. Total gerado: 4065\n",
      "Aumentando classe LSIL com 3543 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████▏                                                 | 355/939 [00:04<00:07, 80.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe LSIL. Total gerado: 3543\n",
      "Aumentando classe HSIL com 3300 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████                                                         | 330/1182 [00:04<00:12, 70.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe HSIL. Total gerado: 3300\n",
      "Aumentando classe SCC com 4370 imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112/112 [00:01<00:00, 60.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumento de dados concluído para classe SCC. Total gerado: 1120\n",
      "Processamento completo com extração, normalização e salvamento para 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mahotas\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage import morphology\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ========== CAMINHOS ==========\n",
    "base_dir = \"E:/datasets/imagens/base\"\n",
    "json_path = os.path.join(base_dir, \"classifications_6classes.json\")\n",
    "output_dir_treino = \"E:/datasets/imagens/treino/treino/6classes/\"\n",
    "output_dir_val = \"E:/datasets/imagens/validacao/validacao/6classes/\"\n",
    "output_dir_teste = \"E:/datasets/imagens/teste/teste/6classes/\"\n",
    "\n",
    "# ========== CRIAR DIRETÓRIOS PARA SISTEMA BETHESDA ==========\n",
    "\n",
    "# Diretórios de treino (RGB)\n",
    "train_negative_dir_rgb = os.path.join(output_dir_treino, \"treino-dir-NEGATIVE-rgb\")\n",
    "train_asch_dir_rgb     = os.path.join(output_dir_treino, \"treino-dir-ASC-H-rgb\")\n",
    "train_ascus_dir_rgb    = os.path.join(output_dir_treino, \"treino-dir-ASC-US-rgb\")\n",
    "train_lsil_dir_rgb     = os.path.join(output_dir_treino, \"treino-dir-LSIL-rgb\")\n",
    "train_hsil_dir_rgb     = os.path.join(output_dir_treino, \"treino-dir-HSIL-rgb\")\n",
    "train_scc_dir_rgb      = os.path.join(output_dir_treino, \"treino-dir-SCC-rgb\")\n",
    "\n",
    "# Diretórios de validação (RGB)\n",
    "val_negative_dir_rgb = os.path.join(output_dir_val, \"validacao-dir-NEGATIVE-rgb\")\n",
    "val_asch_dir_rgb     = os.path.join(output_dir_val, \"validacao-dir-ASC-H-rgb\")\n",
    "val_ascus_dir_rgb    = os.path.join(output_dir_val, \"validacao-dir-ASC-US-rgb\")\n",
    "val_lsil_dir_rgb     = os.path.join(output_dir_val, \"validacao-dir-LSIL-rgb\")\n",
    "val_hsil_dir_rgb     = os.path.join(output_dir_val, \"validacao-dir-HSIL-rgb\")\n",
    "val_scc_dir_rgb      = os.path.join(output_dir_val, \"validacao-dir-SCC-rgb\")\n",
    "\n",
    "# Diretórios de teste (RGB)\n",
    "test_negative_dir_rgb = os.path.join(output_dir_teste, \"teste-dir-NEGATIVE-rgb\")\n",
    "test_asch_dir_rgb     = os.path.join(output_dir_teste, \"teste-dir-ASC-H-rgb\")\n",
    "test_ascus_dir_rgb    = os.path.join(output_dir_teste, \"teste-dir-ASC-US-rgb\")\n",
    "test_lsil_dir_rgb     = os.path.join(output_dir_teste, \"teste-dir-LSIL-rgb\")\n",
    "test_hsil_dir_rgb     = os.path.join(output_dir_teste, \"teste-dir-HSIL-rgb\")\n",
    "test_scc_dir_rgb      = os.path.join(output_dir_teste, \"teste-dir-SCC-rgb\")\n",
    "\n",
    "# Diretórios de treino (grayscale)\n",
    "train_negative_dir = os.path.join(output_dir_treino, \"treino-dir-NEGATIVE\")\n",
    "train_asch_dir     = os.path.join(output_dir_treino, \"treino-dir-ASC-H\")\n",
    "train_ascus_dir    = os.path.join(output_dir_treino, \"treino-dir-ASC-US\")\n",
    "train_lsil_dir     = os.path.join(output_dir_treino, \"treino-dir-LSIL\")\n",
    "train_hsil_dir     = os.path.join(output_dir_treino, \"treino-dir-HSIL\")\n",
    "train_scc_dir      = os.path.join(output_dir_treino, \"treino-dir-SCC\")\n",
    "\n",
    "# Diretórios de validação (grayscale)\n",
    "val_negative_dir = os.path.join(output_dir_val, \"validacao-dir-NEGATIVE\")\n",
    "val_asch_dir     = os.path.join(output_dir_val, \"validacao-dir-ASC-H\")\n",
    "val_ascus_dir    = os.path.join(output_dir_val, \"validacao-dir-ASC-US\")\n",
    "val_lsil_dir     = os.path.join(output_dir_val, \"validacao-dir-LSIL\")\n",
    "val_hsil_dir     = os.path.join(output_dir_val, \"validacao-dir-HSIL\")\n",
    "val_scc_dir      = os.path.join(output_dir_val, \"validacao-dir-SCC\")\n",
    "\n",
    "# Diretórios de teste (grayscale)\n",
    "test_negative_dir = os.path.join(output_dir_teste, \"teste-dir-NEGATIVE\")\n",
    "test_asch_dir     = os.path.join(output_dir_teste, \"teste-dir-ASC-H\")\n",
    "test_ascus_dir    = os.path.join(output_dir_teste, \"teste-dir-ASC-US\")\n",
    "test_lsil_dir     = os.path.join(output_dir_teste, \"teste-dir-LSIL\")\n",
    "test_hsil_dir     = os.path.join(output_dir_teste, \"teste-dir-HSIL\")\n",
    "test_scc_dir      = os.path.join(output_dir_teste, \"teste-dir-SCC\")\n",
    "\n",
    "# ========== CRIAR OS DIRETÓRIOS ==========\n",
    "os.makedirs(train_negative_dir_rgb, exist_ok=True)\n",
    "os.makedirs(train_asch_dir_rgb, exist_ok=True)\n",
    "os.makedirs(train_ascus_dir_rgb, exist_ok=True)\n",
    "os.makedirs(train_lsil_dir_rgb, exist_ok=True)\n",
    "os.makedirs(train_hsil_dir_rgb, exist_ok=True)\n",
    "os.makedirs(train_scc_dir_rgb, exist_ok=True)\n",
    "\n",
    "os.makedirs(val_negative_dir_rgb, exist_ok=True)\n",
    "os.makedirs(val_asch_dir_rgb, exist_ok=True)\n",
    "os.makedirs(val_ascus_dir_rgb, exist_ok=True)\n",
    "os.makedirs(val_lsil_dir_rgb, exist_ok=True)\n",
    "os.makedirs(val_hsil_dir_rgb, exist_ok=True)\n",
    "os.makedirs(val_scc_dir_rgb, exist_ok=True)\n",
    "\n",
    "os.makedirs(test_negative_dir_rgb, exist_ok=True)\n",
    "os.makedirs(test_asch_dir_rgb, exist_ok=True)\n",
    "os.makedirs(test_ascus_dir_rgb, exist_ok=True)\n",
    "os.makedirs(test_lsil_dir_rgb, exist_ok=True)\n",
    "os.makedirs(test_hsil_dir_rgb, exist_ok=True)\n",
    "os.makedirs(test_scc_dir_rgb, exist_ok=True)\n",
    "\n",
    "os.makedirs(train_negative_dir, exist_ok=True)\n",
    "os.makedirs(train_asch_dir, exist_ok=True)\n",
    "os.makedirs(train_ascus_dir, exist_ok=True)\n",
    "os.makedirs(train_lsil_dir, exist_ok=True)\n",
    "os.makedirs(train_hsil_dir, exist_ok=True)\n",
    "os.makedirs(train_scc_dir, exist_ok=True)\n",
    "\n",
    "os.makedirs(val_negative_dir, exist_ok=True)\n",
    "os.makedirs(val_asch_dir, exist_ok=True)\n",
    "os.makedirs(val_ascus_dir, exist_ok=True)\n",
    "os.makedirs(val_lsil_dir, exist_ok=True)\n",
    "os.makedirs(val_hsil_dir, exist_ok=True)\n",
    "os.makedirs(val_scc_dir, exist_ok=True)\n",
    "\n",
    "os.makedirs(test_negative_dir, exist_ok=True)\n",
    "os.makedirs(test_asch_dir, exist_ok=True)\n",
    "os.makedirs(test_ascus_dir, exist_ok=True)\n",
    "os.makedirs(test_lsil_dir, exist_ok=True)\n",
    "os.makedirs(test_hsil_dir, exist_ok=True)\n",
    "os.makedirs(test_scc_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========== CARREGAR JSON ==========\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ========== EXTRATO DE CÉLULAS ==========\n",
    "all_cells = []\n",
    "for img_data in data:\n",
    "    image_name = img_data[\"image_name\"]\n",
    "    for cell in img_data[\"classifications\"]:\n",
    "        all_cells.append({\n",
    "            \"image_name\": image_name,\n",
    "            \"cell_id\": cell[\"cell_id\"],\n",
    "            \"x\": cell[\"nucleus_x\"],\n",
    "            \"y\": cell[\"nucleus_y\"],\n",
    "            \"label\": cell[\"bethesda_system\"]\n",
    "        })\n",
    "\n",
    "# ========== DIVISÃO ENTRE AS 6 CLASSES ==========\n",
    "negative_cells = [c for c in all_cells if c[\"label\"] == \"NEGATIVE\"]\n",
    "asch_cells     = [c for c in all_cells if c[\"label\"] == \"ASC-H\"]\n",
    "ascus_cells    = [c for c in all_cells if c[\"label\"] == \"ASC-US\"]\n",
    "lsil_cells     = [c for c in all_cells if c[\"label\"] == \"LSIL\"]\n",
    "hsil_cells     = [c for c in all_cells if c[\"label\"] == \"HSIL\"]\n",
    "scc_cells      = [c for c in all_cells if c[\"label\"] == \"SCC\"]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(negative_cells)\n",
    "random.shuffle(asch_cells)\n",
    "random.shuffle(ascus_cells)\n",
    "random.shuffle(lsil_cells)\n",
    "random.shuffle(hsil_cells)\n",
    "random.shuffle(scc_cells)\n",
    "\n",
    "def split_data(cells, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = len(cells)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = train_end + int(n * val_ratio)\n",
    "    return cells[:train_end], cells[train_end:val_end], cells[val_end:]\n",
    "\n",
    "# Inicializar contador\n",
    "descartadas = 0\n",
    "\n",
    "# Dividir dados\n",
    "train_negative_cells, val_negative_cells, test_negative_cells = split_data(negative_cells)\n",
    "train_asch_cells, val_asch_cells, test_asch_cells = split_data(asch_cells)\n",
    "train_ascus_cells, val_ascus_cells, test_ascus_cells = split_data(ascus_cells)\n",
    "train_lsil_cells, val_lsil_cells, test_lsil_cells = split_data(lsil_cells)\n",
    "train_hsil_cells, val_hsil_cells, test_hsil_cells = split_data(hsil_cells)\n",
    "train_scc_cells, val_scc_cells, test_scc_cells = split_data(scc_cells)\n",
    "\n",
    "# ========== DICIONÁRIOS USADAS E DIRETÓRIOS ==========\n",
    "usadas = dict.fromkeys([\n",
    "    \"train_negative\", \"val_negative\", \"test_negative\",\n",
    "    \"train_asch\", \"val_asch\", \"test_asch\",\n",
    "    \"train_ascus\", \"val_ascus\", \"test_ascus\",\n",
    "    \"train_lsil\", \"val_lsil\", \"test_lsil\",\n",
    "    \"train_hsil\", \"val_hsil\", \"test_hsil\",\n",
    "    \"train_scc\", \"val_scc\", \"test_scc\"\n",
    "], 0)\n",
    "\n",
    "gray_dirs = {\n",
    "    \"train_negative\": train_negative_dir,\n",
    "    \"val_negative\": val_negative_dir,\n",
    "    \"test_negative\": test_negative_dir,\n",
    "    \"train_asch\": train_asch_dir,\n",
    "    \"val_asch\": val_asch_dir,\n",
    "    \"test_asch\": test_asch_dir,\n",
    "    \"train_ascus\": train_ascus_dir,\n",
    "    \"val_ascus\": val_ascus_dir,\n",
    "    \"test_ascus\": test_ascus_dir,\n",
    "    \"train_lsil\": train_lsil_dir,\n",
    "    \"val_lsil\": val_lsil_dir,\n",
    "    \"test_lsil\": test_lsil_dir,\n",
    "    \"train_hsil\": train_hsil_dir,\n",
    "    \"val_hsil\": val_hsil_dir,\n",
    "    \"test_hsil\": test_hsil_dir,\n",
    "    \"train_scc\": train_scc_dir,\n",
    "    \"val_scc\": val_scc_dir,\n",
    "    \"test_scc\": test_scc_dir,\n",
    "}\n",
    "\n",
    "rgb_dirs = {\n",
    "    \"train_negative\": train_negative_dir_rgb,\n",
    "    \"val_negative\": val_negative_dir_rgb,\n",
    "    \"test_negative\": test_negative_dir_rgb,\n",
    "    \"train_asch\": train_asch_dir_rgb,\n",
    "    \"val_asch\": val_asch_dir_rgb,\n",
    "    \"test_asch\": test_asch_dir_rgb,\n",
    "    \"train_ascus\": train_ascus_dir_rgb,\n",
    "    \"val_ascus\": val_ascus_dir_rgb,\n",
    "    \"test_ascus\": test_ascus_dir_rgb,\n",
    "    \"train_lsil\": train_lsil_dir_rgb,\n",
    "    \"val_lsil\": val_lsil_dir_rgb,\n",
    "    \"test_lsil\": test_lsil_dir_rgb,\n",
    "    \"train_hsil\": train_hsil_dir_rgb,\n",
    "    \"val_hsil\": val_hsil_dir_rgb,\n",
    "    \"test_hsil\": test_hsil_dir_rgb,\n",
    "    \"train_scc\": train_scc_dir_rgb,\n",
    "    \"val_scc\": val_scc_dir_rgb,\n",
    "    \"test_scc\": test_scc_dir_rgb,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def save_cropped_dual(cell, image_dir, key):\n",
    "    global descartadas\n",
    "    image_path = os.path.join(image_dir, cell[\"image_name\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        descartadas += 1\n",
    "        return\n",
    "    try:\n",
    "        # Abrir em RGB e em escala de cinza\n",
    "        img_rgb = Image.open(image_path).convert(\"RGB\")\n",
    "        img_gray = img_rgb.convert(\"L\")\n",
    "    except:\n",
    "        descartadas += 1\n",
    "        return\n",
    "    x, y = cell[\"x\"], cell[\"y\"]\n",
    "    half_crop = 35\n",
    "    if x - half_crop < 0 or y - half_crop < 0 or x + half_crop > img_rgb.width or y + half_crop > img_rgb.height:\n",
    "        descartadas += 1\n",
    "        return\n",
    "\n",
    "    # Recortar ambas as versões\n",
    "    crop_rgb = img_rgb.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "    crop_gray = img_gray.crop((x - half_crop, y - half_crop, x + half_crop, y + half_crop))\n",
    "\n",
    "    name = f\"{os.path.splitext(cell['image_name'])[0]}_celula_{cell['cell_id']}.png\"\n",
    "\n",
    "    # Salvar\n",
    "    crop_rgb.save(os.path.join(rgb_dirs[key], name))\n",
    "    crop_gray.save(os.path.join(gray_dirs[key], name))\n",
    "    usadas[key] += 1\n",
    "\n",
    "# Salvando as imagens cortadas\n",
    "# NEGATIVE\n",
    "# NEGATIVE\n",
    "for c in train_negative_cells: save_cropped_dual(c, base_dir, \"train_negative\")\n",
    "for c in val_negative_cells: save_cropped_dual(c, base_dir, \"val_negative\")\n",
    "for c in test_negative_cells: save_cropped_dual(c, base_dir, \"test_negative\")\n",
    "\n",
    "# ASC-H\n",
    "for c in train_asch_cells: save_cropped_dual(c, base_dir, \"train_asch\")\n",
    "for c in val_asch_cells: save_cropped_dual(c, base_dir, \"val_asch\")\n",
    "for c in test_asch_cells: save_cropped_dual(c, base_dir, \"test_asch\")\n",
    "\n",
    "# ASC-US\n",
    "for c in train_ascus_cells: save_cropped_dual(c, base_dir, \"train_ascus\")\n",
    "for c in val_ascus_cells: save_cropped_dual(c, base_dir, \"val_ascus\")\n",
    "for c in test_ascus_cells: save_cropped_dual(c, base_dir, \"test_ascus\")\n",
    "\n",
    "# LSIL\n",
    "for c in train_lsil_cells: save_cropped_dual(c, base_dir, \"train_lsil\")\n",
    "for c in val_lsil_cells: save_cropped_dual(c, base_dir, \"val_lsil\")\n",
    "for c in test_lsil_cells: save_cropped_dual(c, base_dir, \"test_lsil\")\n",
    "\n",
    "# HSIL\n",
    "for c in train_hsil_cells: save_cropped_dual(c, base_dir, \"train_hsil\")\n",
    "for c in val_hsil_cells: save_cropped_dual(c, base_dir, \"val_hsil\")\n",
    "for c in test_hsil_cells: save_cropped_dual(c, base_dir, \"test_hsil\")\n",
    "\n",
    "# SCC\n",
    "for c in train_scc_cells: save_cropped_dual(c, base_dir, \"train_scc\")\n",
    "for c in val_scc_cells: save_cropped_dual(c, base_dir, \"val_scc\")\n",
    "for c in test_scc_cells: save_cropped_dual(c, base_dir, \"test_scc\")\n",
    "\n",
    "print(f\"Células descartadas: {descartadas}\")\n",
    "\n",
    "\n",
    "# ========== TRANSFORMAÇÕES ==========\n",
    "def apply_augmentations(img):\n",
    "    return [\n",
    "        img.rotate(90),\n",
    "        img.rotate(180),\n",
    "        img.rotate(270),\n",
    "        img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "        img.transpose(Image.FLIP_TOP_BOTTOM),\n",
    "        ImageEnhance.Contrast(img).enhance(1.5),\n",
    "        ImageEnhance.Sharpness(img).enhance(2),\n",
    "        img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "        img.filter(ImageFilter.MedianFilter(size=3)),\n",
    "        img.rotate(15)\n",
    "    ]\n",
    "\n",
    "def balancear_treinamento_automaticamente(\n",
    "    negative_dir, asch_dir, ascus_dir, lsil_dir, hsil_dir, scc_dir\n",
    "):\n",
    "    neg_files = [f for f in os.listdir(negative_dir) if f.endswith(\".png\")]\n",
    "    asch_files = [f for f in os.listdir(asch_dir) if f.endswith(\".png\")]\n",
    "    ascus_files = [f for f in os.listdir(ascus_dir) if f.endswith(\".png\")]\n",
    "    lsil_files = [f for f in os.listdir(lsil_dir) if f.endswith(\".png\")]\n",
    "    hsil_files = [f for f in os.listdir(hsil_dir) if f.endswith(\".png\")]\n",
    "    scc_files = [f for f in os.listdir(scc_dir) if f.endswith(\".png\")]\n",
    "\n",
    "    qtd_neg = len(neg_files)\n",
    "    qtd_asch = len(asch_files)\n",
    "    qtd_ascus = len(ascus_files)\n",
    "    qtd_lsil = len(lsil_files)\n",
    "    qtd_hsil = len(hsil_files)\n",
    "    qtd_scc = len(scc_files)\n",
    "\n",
    "    # Identificar a quantidade máxima entre as classes\n",
    "    max_qtd = max(qtd_neg, qtd_asch, qtd_ascus, qtd_lsil, qtd_hsil, qtd_scc)\n",
    "\n",
    "    # Lista de classes e seus dados\n",
    "    classes = [\n",
    "        (\"NEGATIVE\", qtd_neg, negative_dir, neg_files),\n",
    "        (\"ASC-H\", qtd_asch, asch_dir, asch_files),\n",
    "        (\"ASC-US\", qtd_ascus, ascus_dir, ascus_files),\n",
    "        (\"LSIL\", qtd_lsil, lsil_dir, lsil_files),\n",
    "        (\"HSIL\", qtd_hsil, hsil_dir, hsil_files),\n",
    "        (\"SCC\", qtd_scc, scc_dir, scc_files),\n",
    "    ]\n",
    "\n",
    "    # Para cada classe com quantidade menor que o máximo, aplicar aumento\n",
    "    for classe, qtd, base_dir, base_files in classes:\n",
    "        if qtd < max_qtd:\n",
    "            deficit = max_qtd - qtd\n",
    "            print(f\"Aumentando classe {classe} com {deficit} imagens...\")\n",
    "\n",
    "            contador = 0\n",
    "            for f in tqdm(base_files):\n",
    "                if contador >= deficit:\n",
    "                    break\n",
    "                path = os.path.join(base_dir, f)\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"L\")\n",
    "                    for i, aug in enumerate(apply_augmentations(img)):\n",
    "                        if contador >= deficit:\n",
    "                            break\n",
    "                        out_name = f\"{os.path.splitext(f)[0]}_aug{i+1}.png\"\n",
    "                        aug.save(os.path.join(base_dir, out_name))\n",
    "                        contador += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar {f}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"Aumento de dados concluído para classe {classe}. Total gerado: {contador}\")\n",
    "        else:\n",
    "            print(f\"Classe {classe} já está balanceada.\")\n",
    "\n",
    "\n",
    "# Chamada da função para os diretórios das 6 classes no treino\n",
    "balancear_treinamento_automaticamente(\n",
    "    train_negative_dir,\n",
    "    train_asch_dir,\n",
    "    train_ascus_dir,\n",
    "    train_lsil_dir,\n",
    "    train_hsil_dir,\n",
    "    train_scc_dir,\n",
    ")\n",
    "# ========== EXTRAÇÃO DE ATRIBUTOS ==========\n",
    "def extrair_atributos(p):\n",
    "    img = imread(p, as_gray=True)\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "\n",
    "    try:\n",
    "        bin = morphology.remove_small_objects(img > threshold_otsu(img), 30)\n",
    "        props = regionprops(label(bin))\n",
    "        if props:\n",
    "            p = props[0]\n",
    "            area, perim = p.area, p.perimeter\n",
    "            ecc = p.eccentricity\n",
    "            circ = 4*np.pi*area/(perim**2) if perim > 0 else 0\n",
    "            elip = p.major_axis_length/p.minor_axis_length if p.minor_axis_length > 0 else 0\n",
    "        else:\n",
    "            area = perim = ecc = circ = elip = 0\n",
    "    except:\n",
    "        area = perim = ecc = circ = elip = 0\n",
    "\n",
    "    mean, std, skw, krt = img.mean(), img.std(), skew(img.ravel()), kurtosis(img.ravel())\n",
    "    ent = -np.sum(img * np.log2(img + 1e-10))\n",
    "\n",
    "    glcm = graycomatrix(img_u8, [1], [0], symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    corr = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "    homog = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "\n",
    "    lbp = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "\n",
    "    hrlk = mahotas.features.haralick(img_u8).mean(axis=0)\n",
    "    tas = mahotas.features.tas(img_u8)\n",
    "    zern = mahotas.features.zernike_moments(img_u8, radius=min(img.shape)//2, degree=8)\n",
    "\n",
    "    # ===== DESCRITORES DE FOURIER =====\n",
    "    fft = np.fft.fft2(img)\n",
    "    fft_shift = np.fft.fftshift(fft)\n",
    "    magnitude_spectrum = np.abs(fft_shift)\n",
    "    magnitude_spectrum /= (magnitude_spectrum.max() + 1e-10)  # Normalização\n",
    "\n",
    "    fft_mean = magnitude_spectrum.mean()\n",
    "    fft_std = magnitude_spectrum.std()\n",
    "    fft_energy = np.sum(magnitude_spectrum**2)\n",
    "    fft_entropy = -np.sum(magnitude_spectrum * np.log2(magnitude_spectrum + 1e-10))\n",
    "\n",
    "    return np.hstack([\n",
    "        area, perim, ecc, circ, elip,\n",
    "        mean, std, skw, krt, ent,\n",
    "        contrast, corr, energy, homog,\n",
    "        lbp_hist, hrlk, tas, zern, fft_mean, fft_std, fft_energy, fft_entropy\n",
    "    ])\n",
    "\n",
    "# ========== CSV COM NORMALIZAÇÃO BASEADA NO TREINO ==========\n",
    "def gerar_df_csv(diretorio, label):\n",
    "    linhas = []\n",
    "    for arq in os.listdir(diretorio):\n",
    "        if arq.endswith(\".png\"):\n",
    "            path = os.path.normpath(os.path.join(diretorio, arq))\n",
    "            feat = extrair_atributos(path)\n",
    "            base = os.path.splitext(arq)[0].split(\"_celula_\")\n",
    "            linhas.append([base[0], base[1] if len(base) > 1 else \"NA\"] + list(feat) + [label])\n",
    "    df = pd.DataFrame(linhas)\n",
    "    df.columns = [\"image_name\", \"cell_id\"] + [f\"feat_{i}\" for i in range(len(linhas[0]) - 3)] + [\"label\"]\n",
    "    return df\n",
    "\n",
    "def normalizar_e_salvar(df_train, df_val, df_test):\n",
    "    col_attr = df_train.columns[2:-1]\n",
    "    scaler = MinMaxScaler().fit(df_train[col_attr])\n",
    "    df_train[col_attr] = scaler.transform(df_train[col_attr])\n",
    "    df_val[col_attr] = scaler.transform(df_val[col_attr])\n",
    "    df_test[col_attr] = scaler.transform(df_test[col_attr])\n",
    "    df_train.to_csv(\"train_6classes.csv\", index=False)\n",
    "    df_val.to_csv(\"val_6classes.csv\", index=False)\n",
    "    df_test.to_csv(\"test_6classes.csv\", index=False)\n",
    "\n",
    "# === Gerar dataframes unindo as 6 classes ===\n",
    "df_train = pd.concat([\n",
    "    gerar_df_csv(train_negative_dir, 0),\n",
    "    gerar_df_csv(train_asch_dir, 1),\n",
    "    gerar_df_csv(train_ascus_dir, 2),\n",
    "    gerar_df_csv(train_lsil_dir, 3),\n",
    "    gerar_df_csv(train_hsil_dir, 4),\n",
    "    gerar_df_csv(train_scc_dir, 5),\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_val = pd.concat([\n",
    "    gerar_df_csv(val_negative_dir, 0),\n",
    "    gerar_df_csv(val_asch_dir, 1),\n",
    "    gerar_df_csv(val_ascus_dir, 2),\n",
    "    gerar_df_csv(val_lsil_dir, 3),\n",
    "    gerar_df_csv(val_hsil_dir, 4),\n",
    "    gerar_df_csv(val_scc_dir, 5),\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_test = pd.concat([\n",
    "    gerar_df_csv(test_negative_dir, 0),\n",
    "    gerar_df_csv(test_asch_dir, 1),\n",
    "    gerar_df_csv(test_ascus_dir, 2),\n",
    "    gerar_df_csv(test_lsil_dir, 3),\n",
    "    gerar_df_csv(test_hsil_dir, 4),\n",
    "    gerar_df_csv(test_scc_dir, 5),\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "normalizar_e_salvar(df_train, df_val, df_test)\n",
    "\n",
    "print(\"Processamento completo com extração, normalização e salvamento para 6 classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a320be-e11c-49ab-a56c-1dc39a6fab63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
